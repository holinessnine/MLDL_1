{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDxtMC2g66gQ"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vj2CXov7JJqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "225490e9-6649-4842-8ed5-6cf8d3569d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DcKp4bZiJwut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d42052c-d41c-46f4-fe0d-2d9cfa5c8d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MLDL_lab/hw4\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Change directory to where this file is located\n",
        "\"\"\"\n",
        "%cd '/content/drive/MyDrive/MLDL_lab/hw4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UHKo6dP6eEO6"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "from data.data import prepareData\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Discussed with: 강창훈(데이터사이언스학과)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "5U2jKstSrVfj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6c17f3d4-bf93-4959-e7fe-7be1a8651309"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDiscussed with: 강창훈(데이터사이언스학과)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz2VM-9MIyKe"
      },
      "source": [
        "## Util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qCY42PE0kRj"
      },
      "source": [
        "**Do NOT Modify** code blocks in this section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OOnSsL1EeG85"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UHo7XkIGIz2z"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, loss_fn, clip):    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch[0].to(DEVICE)\n",
        "        trg = batch[1].to(DEVICE)        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        loss = loss_fn(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) \n",
        "        optimizer.step()        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l2-jeXr-I1yP"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, loss_fn):    \n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():    \n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch[0].to(DEVICE)\n",
        "            trg = batch[1].to(DEVICE)\n",
        "            output = model(src, trg)\n",
        "            loss = loss_fn(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    plt.figure(figsize=(2 * 13, 4))\n",
        "    plt.subplot(1, 5, 1)\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.plot(history['train_PPL'], label=\"train_PPL\")\n",
        "    plt.plot(history['val_PPL'], label=\"val_PPL\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"PPL\")\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 5, 2)\n",
        "    plt.title(\"Learning Rate\")\n",
        "    plt.plot(history['lr'], label=\"learning rate\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"LR\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gnb-nEET7aEe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PgCsIyawXoN"
      },
      "source": [
        "## Dataset & Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40ZOR80S0wet"
      },
      "source": [
        "**Do NOT Modify** code blocks in this section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jwwJGiyETHsL"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "TRAIN_RATIO = 0.7 # train dataset ratio, should be a float in (0, 0.8]\n",
        "VALID_RATIO = 0.8 - TRAIN_RATIO\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fePBsU2GKoaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc4e66b-833c-4575-8298-4e2d293fed11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4345\n",
            "eng 2803\n",
            "\n",
            "\n",
            "This is data example\n",
            "['tu me fais de l ombre .', 'you re blocking my light .']\n",
            "\n",
            "\n",
            "This is index of PAD token for each language\n",
            "fra 2803\n",
            "eng 4345\n",
            "\n",
            "\n",
            "This is dataset_size: 10599\n",
            "train_size: 7419\n",
            "valid_data: 1059\n",
            "test_data: 2121\n"
          ]
        }
      ],
      "source": [
        "class TranslateDataset(Dataset):\n",
        "    def __init__(self, max_length=10, fra2eng=True):\n",
        "        self.input_lang, self.output_lang, self.pairs = prepareData('eng', 'fra', max_length=max_length, reverse=fra2eng)\n",
        "        self.max_length = max_length\n",
        "        self.input_lang.addWord('PAD')\n",
        "        self.output_lang.addWord('PAD')\n",
        "        self.input_lang_pad = self.input_lang.word2index['PAD']\n",
        "        self.output_lang_pad = self.output_lang.word2index['PAD']\n",
        "        \n",
        "        print(\"\\n\")\n",
        "        print(\"This is data example\")\n",
        "        print(random.choice(self.pairs))\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"This is index of PAD token for each language\")\n",
        "        print(f\"fra {self.output_lang.word2index['PAD']}\")\n",
        "        print(f\"eng {self.input_lang.word2index['PAD']}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs[idx]\n",
        "        x, y = self._tensorsFromPair(pair)\n",
        "        return x, y\n",
        "\n",
        "    def _tensorFromSentence(self, lang, sentence):\n",
        "        indexes = [lang.word2index[word] for word in sentence.split(' ')]\n",
        "        indexes.append(EOS_token)\n",
        "        return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
        "\n",
        "    def _tensorsFromPair(self, pair):\n",
        "        input_tensor = self._tensorFromSentence(self.input_lang, pair[0])\n",
        "        target_tensor = self._tensorFromSentence(self.output_lang, pair[1])\n",
        "        return (input_tensor, target_tensor)\n",
        "    \n",
        "    def collate_fn(self, data):\n",
        "        x_batch = []; y_batch = []\n",
        "        \n",
        "        for x, y in data:\n",
        "            if x.shape[0] < self.max_length-1:\n",
        "                x = torch.cat([x, self.input_lang_pad*torch.ones((self.max_length-1 - x.shape[0], 1), dtype=x.dtype)])\n",
        "            elif x.shape[0] > self.max_length-1:\n",
        "                x = x[:self.max_length-1]\n",
        "            if y.shape[0] < self.max_length-1:\n",
        "                y = torch.cat([y, self.output_lang_pad*torch.ones((self.max_length-1 - y.shape[0], 1), dtype=y.dtype)])\n",
        "            elif y.shape[0] > self.max_length-1:\n",
        "                y = y[:self.max_length-1]\n",
        "\n",
        "            x_batch.append(torch.cat([torch.tensor([SOS_token]), x.squeeze(1)]))\n",
        "            y_batch.append(torch.cat([torch.tensor([SOS_token]), y.squeeze(1)]))\n",
        "        \n",
        "        return torch.stack(x_batch), torch.stack(y_batch)\n",
        "\n",
        "dataset = TranslateDataset(max_length=MAX_LENGTH)\n",
        "\n",
        "train_size = int(len(dataset)*TRAIN_RATIO)\n",
        "valid_size = int(len(dataset)*VALID_RATIO)\n",
        "train_data, valid_data, test_data = random_split(dataset, [train_size, valid_size, len(dataset)-(train_size+valid_size)],)\n",
        "print(\"\\n\")\n",
        "print(f\"This is dataset_size: {len(dataset)}\")\n",
        "print(f\"train_size: {train_size}\")\n",
        "print(f\"valid_data: {valid_size}\")\n",
        "print(f\"test_data: {len(test_data)}\")\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TqutY58tG-h"
      },
      "source": [
        "# 1. Seq2Seq model with Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5xyf2mHuhmX"
      },
      "source": [
        "## Implement LSTM Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "1MM6lL95JcDa"
      },
      "outputs": [],
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_dim, emb_dim, hid_dim):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(in_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hid_dim, num_layers=1, batch_first=True)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        '''\n",
        "        Q2 - (a)\n",
        "        Implement forward method of LSTM Encoder Module\n",
        "\n",
        "        INPUT\n",
        "        - input: input sentence, (B, max_len)\n",
        "        - hidden: initialized hidden state, (1, B, hid_dim)\n",
        "        - cell: initialized cell state, (1, B, hid_dim)\n",
        "\n",
        "        OUTPUT\n",
        "        What to be returned depends on your implementation of LSTMSeq2Seq. (Q2 - (b))\n",
        "        Feel free to return outputs you need. (e.g. hidden states of encoder, etc.)\n",
        "        '''\n",
        "        ################### YOUR CODE ###################\n",
        "        input = self.embedding(input).to(DEVICE)\n",
        "        hiddens, (_, _) = self.lstm(input, (hidden, cell))\n",
        "        return hiddens\n",
        "        #################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "VPQnZdz5KKnN"
      },
      "outputs": [],
      "source": [
        "class AttnLSTMDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim, hid_dim, out_dim, dropout, enc_hiddens=None):\n",
        "        super(AttnLSTMDecoder, self).__init__()\n",
        "        self.enc_hiddens = enc_hiddens # encoder output\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.embedding = nn.Embedding(out_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hid_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hid_dim + hid_dim, hid_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.classifier = nn.Linear(hid_dim, out_dim)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        '''\n",
        "        Q2 - (a)\n",
        "        Implement forward method of LSTM Decoder Module with dot-product attention\n",
        "        Before implementing LSTM layer, make sure to feed the concatenated input into Linear and tanh activation layer.\n",
        "        This will allow the concatenated input to be resized from (B, hid_dim + hid_dim) into (B, hid_dim) \n",
        "\n",
        "        INPUT\n",
        "        - input: a token of input sentence (B, 1)\n",
        "        - hidden: previous hidden state (B, hid_dim)\n",
        "        - cell: previous cell state (1, B, hid_dim)\n",
        "\n",
        "        OUTPUT\n",
        "        What to be returned depends on your implementation of LSTMSeq2Seq. (Q2 - (b))\n",
        "        Feel free to return outputs you need.\n",
        "        Some examples below\n",
        "        - predicted token embedding (N, emb_dim)\n",
        "        - current hidden state\n",
        "        - current cell state\n",
        "        '''\n",
        "\n",
        "        ################### YOUR CODE ###################\n",
        "        query = hidden # set query to calculate attention\n",
        "        key = self.enc_hiddens.permute(1,0,2)\n",
        "        value = self.enc_hiddens.permute(1,0,2)\n",
        "        \n",
        "        input = self.embedding(input)\n",
        "        input = nn.Dropout(self.dropout)(input)\n",
        "\n",
        "        attn_score = torch.matmul(key, query.permute(0,2,1))\n",
        "        attn_coefficient = F.softmax(attn_score, dim=1)\n",
        "        weighted_kv = (value * attn_coefficient)\n",
        "        attn_value = torch.sum(weighted_kv, dim=1)\n",
        "\n",
        "        hidden_concat = torch.cat([query, attn_value.unsqueeze(1)], dim=2)\n",
        "        hidden_concat = self.fc(hidden_concat)\n",
        "        hidden_concat = self.tanh(hidden_concat)\n",
        "        hidden_concat = hidden_concat.permute(1,0,2)\n",
        "\n",
        "        output, (h,c) = self.lstm(input, (hidden_concat, cell))\n",
        "        output = self.classifier(output)\n",
        "        output = F.log_softmax(output)\n",
        "\n",
        "        return output, (h,c)\n",
        "        #################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "zJsJ3p2NLD6c"
      },
      "outputs": [],
      "source": [
        "class LSTMSeq2Seq(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, emb_dim, hid_dim, device, dropout):\n",
        "        super(LSTMSeq2Seq, self).__init__()\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.device = device\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.encoder = LSTMEncoder(in_dim, emb_dim, hid_dim)\n",
        "        self.decoder = AttnLSTMDecoder(emb_dim, hid_dim, out_dim, dropout)\n",
        "        \n",
        "    def forward(self, src, trg):\n",
        "        '''\n",
        "        Q2 - (b)\n",
        "        Implement forward method of LSTM Seq2Seq Module\n",
        "        (Decoder module should attend encoder's outputs using dot product.)\n",
        "        \n",
        "        INPUT\n",
        "        - src: source language batched data (B, max_len)\n",
        "        - trg: target language batched data (B, max_len)\n",
        "\n",
        "        OUTPUT\n",
        "        - output of one-hot prediction (B, out_dim, max_len)\n",
        "        '''\n",
        "        ################### YOUR CODE ###################\n",
        "        batch_size, mx_len = src.shape\n",
        "        # Encoder (start from zero-hidden & zero-cell states)\n",
        "        h_e = torch.zeros(1, batch_size, self.hid_dim).to(self.device)\n",
        "        c_e = torch.zeros(1, batch_size, self.hid_dim).to(self.device)\n",
        "        hiddens = self.encoder(src, h_e, c_e).permute(1,0,2)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder.enc_hiddens = hiddens # set encoder's hidden states\n",
        "        outputs = torch.zeros(mx_len, batch_size, dataset.output_lang.n_words).to(self.device) # to store each decoder's output\n",
        "        h_d = hiddens[-1].unsqueeze(1).to(self.device)\n",
        "        c_d = torch.zeros(1, batch_size, self.hid_dim).to(self.device)\n",
        "        trg = trg.permute(1,0)\n",
        "        for t in range(1, mx_len): # for each t'th token, get decoder outputs\n",
        "            input = trg[t].unsqueeze(1)\n",
        "            output, (h, c) = self.decoder(input, h_d, c_d)\n",
        "            h_d = h.permute(1,0,2)\n",
        "            outputs[t] = output.squeeze(1)\n",
        "  \n",
        "        return outputs.permute(1,2,0)\n",
        "        #################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu3WP3mYw3NV"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "B4YKhRe9d9qC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "faa470b1-f916-4d0d-bafe-d82680fc3173"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nQ2 - (c)\\nTrain your Seq2Seq model and plot perplexities and learning rates. \\nUpon successful training, the test perplexity should be less than 7. \\nBriefly report your hyperparameters and results on test dataset. \\nMake sure your results are printed in your submitted file.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 171
        }
      ],
      "source": [
        "'''\n",
        "Q2 - (c)\n",
        "Train your Seq2Seq model and plot perplexities and learning rates. \n",
        "Upon successful training, the test perplexity should be less than 7. \n",
        "Briefly report your hyperparameters and results on test dataset. \n",
        "Make sure your results are printed in your submitted file.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "1QQgN03XecUV"
      },
      "outputs": [],
      "source": [
        "# experiment various methods for better performance\n",
        "# you can modify the codes in this block\n",
        "in_dim = dataset.input_lang.n_words\n",
        "out_dim = dataset.output_lang.n_words\n",
        "hid_dim = 128\n",
        "emb_dim = 64\n",
        "dropout = 0.5\n",
        "learning_rate = 0.001\n",
        "N_EPOCHS = 40\n",
        "valid_every=1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LSTMSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = dataset.output_lang_pad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "JbSR6BZKf-6L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17d52937-d805-4397-cefc-adc7d3b22e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-169-671b60b9b4b5>:56: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(output)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 5.683 | Train PPL: 293.731\n",
            "==========================\n",
            "\t Val. Loss: 3.746 |  Val. PPL:  42.346\n",
            "Epoch: 02\n",
            "\tTrain Loss: 3.242 | Train PPL:  25.590\n",
            "==========================\n",
            "\t Val. Loss: 3.078 |  Val. PPL:  21.719\n",
            "Epoch: 03\n",
            "\tTrain Loss: 2.776 | Train PPL:  16.053\n",
            "==========================\n",
            "\t Val. Loss: 2.659 |  Val. PPL:  14.285\n",
            "Epoch: 04\n",
            "\tTrain Loss: 2.458 | Train PPL:  11.685\n",
            "==========================\n",
            "\t Val. Loss: 2.485 |  Val. PPL:  12.005\n",
            "Epoch: 05\n",
            "\tTrain Loss: 2.243 | Train PPL:   9.417\n",
            "==========================\n",
            "\t Val. Loss: 2.237 |  Val. PPL:   9.364\n",
            "Epoch: 06\n",
            "\tTrain Loss: 2.075 | Train PPL:   7.961\n",
            "==========================\n",
            "\t Val. Loss: 2.124 |  Val. PPL:   8.363\n",
            "Epoch: 07\n",
            "\tTrain Loss: 1.982 | Train PPL:   7.260\n",
            "==========================\n",
            "\t Val. Loss: 2.050 |  Val. PPL:   7.768\n",
            "Epoch: 08\n",
            "\tTrain Loss: 1.854 | Train PPL:   6.383\n",
            "==========================\n",
            "\t Val. Loss: 1.983 |  Val. PPL:   7.264\n",
            "Epoch: 09\n",
            "\tTrain Loss: 1.791 | Train PPL:   5.993\n",
            "==========================\n",
            "\t Val. Loss: 2.004 |  Val. PPL:   7.418\n",
            "Epoch: 10\n",
            "\tTrain Loss: 1.718 | Train PPL:   5.576\n",
            "==========================\n",
            "\t Val. Loss: 1.868 |  Val. PPL:   6.478\n",
            "Epoch: 11\n",
            "\tTrain Loss: 1.685 | Train PPL:   5.390\n",
            "==========================\n",
            "\t Val. Loss: 1.838 |  Val. PPL:   6.283\n",
            "Epoch: 12\n",
            "\tTrain Loss: 1.641 | Train PPL:   5.162\n",
            "==========================\n",
            "\t Val. Loss: 1.787 |  Val. PPL:   5.974\n",
            "Epoch: 13\n",
            "\tTrain Loss: 1.611 | Train PPL:   5.006\n",
            "==========================\n",
            "\t Val. Loss: 1.747 |  Val. PPL:   5.735\n",
            "Epoch: 14\n",
            "\tTrain Loss: 1.555 | Train PPL:   4.735\n",
            "==========================\n",
            "\t Val. Loss: 1.758 |  Val. PPL:   5.801\n",
            "Epoch: 15\n",
            "\tTrain Loss: 1.507 | Train PPL:   4.512\n",
            "==========================\n",
            "\t Val. Loss: 1.701 |  Val. PPL:   5.481\n",
            "Epoch: 16\n",
            "\tTrain Loss: 1.488 | Train PPL:   4.430\n",
            "==========================\n",
            "\t Val. Loss: 1.672 |  Val. PPL:   5.323\n",
            "Epoch: 17\n",
            "\tTrain Loss: 1.457 | Train PPL:   4.293\n",
            "==========================\n",
            "\t Val. Loss: 1.693 |  Val. PPL:   5.438\n",
            "Epoch: 18\n",
            "\tTrain Loss: 1.458 | Train PPL:   4.295\n",
            "==========================\n",
            "\t Val. Loss: 1.625 |  Val. PPL:   5.081\n",
            "Epoch: 19\n",
            "\tTrain Loss: 1.426 | Train PPL:   4.163\n",
            "==========================\n",
            "\t Val. Loss: 1.678 |  Val. PPL:   5.352\n",
            "Epoch: 20\n",
            "\tTrain Loss: 1.409 | Train PPL:   4.092\n",
            "==========================\n",
            "\t Val. Loss: 1.635 |  Val. PPL:   5.129\n",
            "Epoch: 21\n",
            "\tTrain Loss: 1.389 | Train PPL:   4.011\n",
            "==========================\n",
            "\t Val. Loss: 1.643 |  Val. PPL:   5.173\n",
            "Epoch: 22\n",
            "\tTrain Loss: 1.406 | Train PPL:   4.078\n",
            "==========================\n",
            "\t Val. Loss: 1.624 |  Val. PPL:   5.071\n",
            "Epoch: 23\n",
            "\tTrain Loss: 1.383 | Train PPL:   3.985\n",
            "==========================\n",
            "\t Val. Loss: 1.695 |  Val. PPL:   5.445\n",
            "Epoch: 24\n",
            "\tTrain Loss: 1.344 | Train PPL:   3.835\n",
            "==========================\n",
            "\t Val. Loss: 1.665 |  Val. PPL:   5.284\n",
            "Epoch: 25\n",
            "\tTrain Loss: 1.368 | Train PPL:   3.927\n",
            "==========================\n",
            "\t Val. Loss: 1.636 |  Val. PPL:   5.133\n",
            "Epoch: 26\n",
            "\tTrain Loss: 1.332 | Train PPL:   3.787\n",
            "==========================\n",
            "\t Val. Loss: 1.628 |  Val. PPL:   5.096\n",
            "Epoch: 27\n",
            "\tTrain Loss: 1.329 | Train PPL:   3.778\n",
            "==========================\n",
            "\t Val. Loss: 1.621 |  Val. PPL:   5.060\n",
            "Epoch: 28\n",
            "\tTrain Loss: 1.324 | Train PPL:   3.758\n",
            "==========================\n",
            "\t Val. Loss: 1.638 |  Val. PPL:   5.145\n",
            "Epoch: 29\n",
            "\tTrain Loss: 1.304 | Train PPL:   3.684\n",
            "==========================\n",
            "\t Val. Loss: 1.679 |  Val. PPL:   5.358\n",
            "Epoch: 30\n",
            "\tTrain Loss: 1.299 | Train PPL:   3.664\n",
            "==========================\n",
            "\t Val. Loss: 1.649 |  Val. PPL:   5.202\n",
            "Epoch: 31\n",
            "\tTrain Loss: 1.296 | Train PPL:   3.655\n",
            "==========================\n",
            "\t Val. Loss: 1.595 |  Val. PPL:   4.926\n",
            "Epoch: 32\n",
            "\tTrain Loss: 1.289 | Train PPL:   3.629\n",
            "==========================\n",
            "\t Val. Loss: 1.580 |  Val. PPL:   4.856\n",
            "Epoch: 33\n",
            "\tTrain Loss: 1.297 | Train PPL:   3.659\n",
            "==========================\n",
            "\t Val. Loss: 1.626 |  Val. PPL:   5.082\n",
            "Epoch: 34\n",
            "\tTrain Loss: 1.275 | Train PPL:   3.577\n",
            "==========================\n",
            "\t Val. Loss: 1.614 |  Val. PPL:   5.020\n",
            "Epoch: 35\n",
            "\tTrain Loss: 1.269 | Train PPL:   3.557\n",
            "==========================\n",
            "\t Val. Loss: 1.697 |  Val. PPL:   5.457\n",
            "Epoch: 36\n",
            "\tTrain Loss: 1.263 | Train PPL:   3.536\n",
            "==========================\n",
            "\t Val. Loss: 1.613 |  Val. PPL:   5.018\n",
            "Epoch: 37\n",
            "\tTrain Loss: 1.270 | Train PPL:   3.563\n",
            "==========================\n",
            "\t Val. Loss: 1.610 |  Val. PPL:   5.004\n",
            "Epoch: 38\n",
            "\tTrain Loss: 1.246 | Train PPL:   3.475\n",
            "==========================\n",
            "\t Val. Loss: 1.610 |  Val. PPL:   5.002\n",
            "Epoch: 39\n",
            "\tTrain Loss: 1.252 | Train PPL:   3.499\n",
            "==========================\n",
            "\t Val. Loss: 1.611 |  Val. PPL:   5.008\n",
            "Epoch: 40\n",
            "\tTrain Loss: 1.242 | Train PPL:   3.463\n",
            "==========================\n",
            "\t Val. Loss: 1.595 |  Val. PPL:   4.930\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1872x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Train your model \n",
        "# you can modify the codes in this block\n",
        "history = {'train_PPL':[], 'val_PPL':[], 'lr':[]}\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model, train_dataloader, optimizer, loss_fn, 1)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    \n",
        "    if epoch%valid_every==0:\n",
        "        print(\"==========================\")\n",
        "        valid_loss = evaluate(model, valid_dataloader, loss_fn)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            model.decoder.t=0\n",
        "            torch.save(model.state_dict(), 'lstm-attn-model.pt')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "        history['train_PPL'].append(math.exp(train_loss))\n",
        "        history['val_PPL'].append(math.exp(valid_loss))\n",
        "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "plot_history(history) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "hBXKAKZo2lSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4de8d3-efa3-4689-f318-cacc8ed67262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-169-671b60b9b4b5>:56: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(output)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Test. Loss: 1.595 |  Test. PPL:   4.930\n"
          ]
        }
      ],
      "source": [
        "# Test your model\n",
        "torch.save(model.state_dict(), 'lstm-attn-model.pt') \n",
        "loaded_model = LSTMSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\n",
        "loaded_model.load_state_dict(torch.load('lstm-attn-model.pt'))\n",
        "\n",
        "test_loss = evaluate(loaded_model, test_dataloader, loss_fn)\n",
        "print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hyper Parameter]\n",
        "- Learning rate을 크게 하면 너무 빨리 수렴함과 동시에 이후의 학습에서는 loss가 나아지지 않거나 크게 변동하는 현상이 발생하였다. 따라서 보다 작은 값인 0.001로 설정하였다.\n",
        "- Dropout 역시 0.1~0.4 구간보다 0.5를 설정하였을 때 학습이 꾸준히 이루어짐을 발견하였다.\n",
        "- valid_every는 1로 설정하여 매 epoch마다 val loss를 확인하도록 하였다.\n",
        "\n",
        "\n",
        "[Test Result]\n",
        "- Training loss가 epoch이 지남에 따라 감소하는데, Validation loss 역시 서서히 감소하였고, 최종 val_PPL이 과제의 기준치인 7 이내에 도달하였다.\n",
        "- 따라서 Overfitting이 발생했다고 보기는 힘들 것 같다."
      ],
      "metadata": {
        "id": "q3iqv1Lh86G_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD1SYbuKOKGN"
      },
      "source": [
        "## [Bonus] Implement GRU Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei1fgjRveS4n"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Q2 - (d)\n",
        "Change the modules(encoder, decoder) in Seq2Seq model to GRU, and repeat (a)~(c).\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "2U9A9kh3OSWW"
      },
      "outputs": [],
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, emb_dim, hid_dim):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "        ################### YOUR CODE ###################\n",
        "        self.embedding = nn.Embedding(in_dim, emb_dim)\n",
        "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hid_dim, num_layers=1, batch_first=True)\n",
        "        #################################################\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        ################### YOUR CODE ###################\n",
        "        input = self.embedding(input).to(device)\n",
        "        hiddens, hidden = self.gru(input, hidden)\n",
        "        return hiddens\n",
        "        #################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "9-IRYjI6O0xb"
      },
      "outputs": [],
      "source": [
        "class AttnGRUDecoder(nn.Module):\n",
        "    def __init__(self, emb_dim, hid_dim, out_dim, dropout, enc_hiddens=None):\n",
        "        super(AttnGRUDecoder, self).__init__()\n",
        "        ################### YOUR CODE ###################\n",
        "        self.enc_hiddens = enc_hiddens\n",
        "        self.dropout = dropout\n",
        "        self.out_dim = out_dim\n",
        "        self.fc = nn.Linear(hid_dim + hid_dim, hid_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.embedding = nn.Embedding(out_dim, emb_dim)\n",
        "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hid_dim, batch_first=True)\n",
        "        self.classifier = nn.Linear(hid_dim, out_dim)\n",
        "        #################################################\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        ################### YOUR CODE ###################\n",
        "        query = hidden # set query to calculate attention\n",
        "        key = self.enc_hiddens.permute(1,0,2)\n",
        "        value = self.enc_hiddens.permute(1,0,2)\n",
        "        \n",
        "        input = self.embedding(input)\n",
        "        input = nn.Dropout(self.dropout)(input)\n",
        "\n",
        "        attn_score = torch.matmul(key, query.permute(0,2,1))\n",
        "        attn_coefficient = F.softmax(attn_score, dim=1)\n",
        "        weighted_kv = (value * attn_coefficient)\n",
        "        attn_value = torch.sum(weighted_kv, dim=1)\n",
        "\n",
        "        hidden_concat = torch.cat([query, attn_value.unsqueeze(1)], dim=2)\n",
        "        hidden_concat = self.fc(hidden_concat)\n",
        "        hidden_concat = self.tanh(hidden_concat)\n",
        "        hidden_concat = hidden_concat.permute(1,0,2)\n",
        "\n",
        "        output, (h) = self.gru(input, (hidden_concat))\n",
        "        output = self.classifier(output)\n",
        "        output = F.log_softmax(output)\n",
        "\n",
        "        return output, h\n",
        "        #################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "BOOj4_hhP83d"
      },
      "outputs": [],
      "source": [
        "class GRUSeq2Seq(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, emb_dim, hid_dim, device, dropout):\n",
        "        super(GRUSeq2Seq, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.device = device\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.encoder = GRUEncoder(in_dim, emb_dim, hid_dim)\n",
        "        self.decoder = AttnGRUDecoder(emb_dim, hid_dim, out_dim, dropout)\n",
        "        \n",
        "    def forward(self, src, trg):\n",
        "        batch_size, mx_len = src.shape\n",
        "        device = self.device\n",
        "        ################### YOUR CODE ###################\n",
        "        # Encoder\n",
        "        h_e = torch.zeros(1, batch_size, self.hid_dim).to(self.device)\n",
        "        hiddens = self.encoder(src, h_e).permute(1,0,2)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder.enc_hiddens = hiddens # set encoder's hidden states\n",
        "        outputs = torch.zeros(mx_len, batch_size, dataset.output_lang.n_words).to(self.device) # to store each decoder's output\n",
        "        h_d = hiddens[-1].unsqueeze(1).to(self.device)\n",
        "        trg = trg.permute(1,0)\n",
        "        for t in range(1, mx_len): # for each t'th token, get decoder outputs\n",
        "            input = trg[t].unsqueeze(1)\n",
        "            output, h = self.decoder(input, h_d)\n",
        "            h_d = h.permute(1,0,2)\n",
        "            outputs[t] = output.squeeze(1)\n",
        "  \n",
        "        return outputs.permute(1,2,0)\n",
        "        #################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "gqajm5Iq2YNx"
      },
      "outputs": [],
      "source": [
        "# experiment various methods for better performance\n",
        "# you can modify the codes in this block\n",
        "in_dim = dataset.input_lang.n_words\n",
        "out_dim = dataset.output_lang.n_words\n",
        "\n",
        "hid_dim = 128\n",
        "emb_dim = 64\n",
        "dropout = 0.5\n",
        "learning_rate=0.001\n",
        "N_EPOCHS = 40\n",
        "valid_every=1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "gru_model = GRUSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(gru_model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = dataset.output_lang_pad) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "usq_ohj4Qqf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf97d10b-0dbc-45a3-f615-a191963c193b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-177-62e0206e5d3a>:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(output)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 5.488 | Train PPL: 241.768\n",
            "==========================\n",
            "\t Val. Loss: 3.660 |  Val. PPL:  38.869\n",
            "Epoch: 02\n",
            "\tTrain Loss: 3.286 | Train PPL:  26.732\n",
            "==========================\n",
            "\t Val. Loss: 3.173 |  Val. PPL:  23.871\n",
            "Epoch: 03\n",
            "\tTrain Loss: 2.750 | Train PPL:  15.644\n",
            "==========================\n",
            "\t Val. Loss: 2.708 |  Val. PPL:  15.002\n",
            "Epoch: 04\n",
            "\tTrain Loss: 2.514 | Train PPL:  12.360\n",
            "==========================\n",
            "\t Val. Loss: 2.521 |  Val. PPL:  12.441\n",
            "Epoch: 05\n",
            "\tTrain Loss: 2.221 | Train PPL:   9.217\n",
            "==========================\n",
            "\t Val. Loss: 2.499 |  Val. PPL:  12.166\n",
            "Epoch: 06\n",
            "\tTrain Loss: 2.176 | Train PPL:   8.813\n",
            "==========================\n",
            "\t Val. Loss: 2.207 |  Val. PPL:   9.089\n",
            "Epoch: 07\n",
            "\tTrain Loss: 1.978 | Train PPL:   7.227\n",
            "==========================\n",
            "\t Val. Loss: 2.062 |  Val. PPL:   7.863\n",
            "Epoch: 08\n",
            "\tTrain Loss: 1.850 | Train PPL:   6.361\n",
            "==========================\n",
            "\t Val. Loss: 2.004 |  Val. PPL:   7.417\n",
            "Epoch: 09\n",
            "\tTrain Loss: 1.756 | Train PPL:   5.787\n",
            "==========================\n",
            "\t Val. Loss: 1.914 |  Val. PPL:   6.782\n",
            "Epoch: 10\n",
            "\tTrain Loss: 1.692 | Train PPL:   5.432\n",
            "==========================\n",
            "\t Val. Loss: 1.882 |  Val. PPL:   6.564\n",
            "Epoch: 11\n",
            "\tTrain Loss: 1.644 | Train PPL:   5.174\n",
            "==========================\n",
            "\t Val. Loss: 1.826 |  Val. PPL:   6.209\n",
            "Epoch: 12\n",
            "\tTrain Loss: 1.591 | Train PPL:   4.907\n",
            "==========================\n",
            "\t Val. Loss: 1.866 |  Val. PPL:   6.461\n",
            "Epoch: 13\n",
            "\tTrain Loss: 1.555 | Train PPL:   4.734\n",
            "==========================\n",
            "\t Val. Loss: 1.812 |  Val. PPL:   6.121\n",
            "Epoch: 14\n",
            "\tTrain Loss: 1.512 | Train PPL:   4.538\n",
            "==========================\n",
            "\t Val. Loss: 1.813 |  Val. PPL:   6.129\n",
            "Epoch: 15\n",
            "\tTrain Loss: 1.472 | Train PPL:   4.360\n",
            "==========================\n",
            "\t Val. Loss: 1.791 |  Val. PPL:   5.994\n",
            "Epoch: 16\n",
            "\tTrain Loss: 1.468 | Train PPL:   4.342\n",
            "==========================\n",
            "\t Val. Loss: 1.697 |  Val. PPL:   5.456\n",
            "Epoch: 17\n",
            "\tTrain Loss: 1.430 | Train PPL:   4.178\n",
            "==========================\n",
            "\t Val. Loss: 1.689 |  Val. PPL:   5.413\n",
            "Epoch: 18\n",
            "\tTrain Loss: 1.409 | Train PPL:   4.091\n",
            "==========================\n",
            "\t Val. Loss: 1.745 |  Val. PPL:   5.728\n",
            "Epoch: 19\n",
            "\tTrain Loss: 1.394 | Train PPL:   4.031\n",
            "==========================\n",
            "\t Val. Loss: 1.739 |  Val. PPL:   5.693\n",
            "Epoch: 20\n",
            "\tTrain Loss: 1.384 | Train PPL:   3.990\n",
            "==========================\n",
            "\t Val. Loss: 1.721 |  Val. PPL:   5.588\n",
            "Epoch: 21\n",
            "\tTrain Loss: 1.356 | Train PPL:   3.880\n",
            "==========================\n",
            "\t Val. Loss: 1.705 |  Val. PPL:   5.503\n",
            "Epoch: 22\n",
            "\tTrain Loss: 1.358 | Train PPL:   3.889\n",
            "==========================\n",
            "\t Val. Loss: 1.682 |  Val. PPL:   5.378\n",
            "Epoch: 23\n",
            "\tTrain Loss: 1.336 | Train PPL:   3.802\n",
            "==========================\n",
            "\t Val. Loss: 1.646 |  Val. PPL:   5.185\n",
            "Epoch: 24\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.733\n",
            "==========================\n",
            "\t Val. Loss: 1.671 |  Val. PPL:   5.318\n",
            "Epoch: 25\n",
            "\tTrain Loss: 1.329 | Train PPL:   3.777\n",
            "==========================\n",
            "\t Val. Loss: 1.680 |  Val. PPL:   5.364\n",
            "Epoch: 26\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.730\n",
            "==========================\n",
            "\t Val. Loss: 1.682 |  Val. PPL:   5.374\n",
            "Epoch: 27\n",
            "\tTrain Loss: 1.295 | Train PPL:   3.650\n",
            "==========================\n",
            "\t Val. Loss: 1.634 |  Val. PPL:   5.126\n",
            "Epoch: 28\n",
            "\tTrain Loss: 1.292 | Train PPL:   3.642\n",
            "==========================\n",
            "\t Val. Loss: 1.638 |  Val. PPL:   5.147\n",
            "Epoch: 29\n",
            "\tTrain Loss: 1.289 | Train PPL:   3.631\n",
            "==========================\n",
            "\t Val. Loss: 1.658 |  Val. PPL:   5.248\n",
            "Epoch: 30\n",
            "\tTrain Loss: 1.280 | Train PPL:   3.598\n",
            "==========================\n",
            "\t Val. Loss: 1.723 |  Val. PPL:   5.603\n",
            "Epoch: 31\n",
            "\tTrain Loss: 1.271 | Train PPL:   3.564\n",
            "==========================\n",
            "\t Val. Loss: 1.642 |  Val. PPL:   5.166\n",
            "Epoch: 32\n",
            "\tTrain Loss: 1.271 | Train PPL:   3.563\n",
            "==========================\n",
            "\t Val. Loss: 1.634 |  Val. PPL:   5.126\n",
            "Epoch: 33\n",
            "\tTrain Loss: 1.265 | Train PPL:   3.543\n",
            "==========================\n",
            "\t Val. Loss: 1.655 |  Val. PPL:   5.234\n",
            "Epoch: 34\n",
            "\tTrain Loss: 1.277 | Train PPL:   3.587\n",
            "==========================\n",
            "\t Val. Loss: 1.630 |  Val. PPL:   5.102\n",
            "Epoch: 35\n",
            "\tTrain Loss: 1.260 | Train PPL:   3.526\n",
            "==========================\n",
            "\t Val. Loss: 1.654 |  Val. PPL:   5.229\n",
            "Epoch: 36\n",
            "\tTrain Loss: 1.241 | Train PPL:   3.460\n",
            "==========================\n",
            "\t Val. Loss: 1.657 |  Val. PPL:   5.246\n",
            "Epoch: 37\n",
            "\tTrain Loss: 1.249 | Train PPL:   3.486\n",
            "==========================\n",
            "\t Val. Loss: 1.702 |  Val. PPL:   5.483\n",
            "Epoch: 38\n",
            "\tTrain Loss: 1.259 | Train PPL:   3.522\n",
            "==========================\n",
            "\t Val. Loss: 1.635 |  Val. PPL:   5.131\n",
            "Epoch: 39\n",
            "\tTrain Loss: 1.240 | Train PPL:   3.456\n",
            "==========================\n",
            "\t Val. Loss: 1.652 |  Val. PPL:   5.218\n",
            "Epoch: 40\n",
            "\tTrain Loss: 1.229 | Train PPL:   3.419\n",
            "==========================\n",
            "\t Val. Loss: 1.644 |  Val. PPL:   5.177\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1872x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Train your model \n",
        "# you can modify the codes in this block\n",
        "history = {'train_PPL':[], 'val_PPL':[], 'lr':[]}\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(gru_model, train_dataloader, optimizer, loss_fn, 1)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    \n",
        "    if epoch%valid_every==0:\n",
        "        print(\"==========================\")\n",
        "        valid_loss = evaluate(gru_model, valid_dataloader, loss_fn)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            gru_model.decoder.t=0\n",
        "            torch.save(gru_model.state_dict(), 'gru-attn-model.pt')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "        history['train_PPL'].append(math.exp(train_loss))\n",
        "        history['val_PPL'].append(math.exp(valid_loss))\n",
        "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "plot_history(history) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "WrFYlXfiRCSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100df102-a081-47ab-e6f7-567e9979d96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-177-62e0206e5d3a>:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(output)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Test. Loss: 1.644 |  Test. PPL:   5.177\n"
          ]
        }
      ],
      "source": [
        "# Test your model\n",
        "torch.save(gru_model.state_dict(), 'gru-attn-model.pt') \n",
        "loaded_model = GRUSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\n",
        "gru_model.load_state_dict(torch.load('gru-attn-model.pt'))\n",
        "\n",
        "test_loss = evaluate(gru_model, test_dataloader, loss_fn)\n",
        "print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAZdJqTJXVcP"
      },
      "source": [
        "# 2. Seq2Seq model with Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t07UOwbpH7CZ"
      },
      "source": [
        "## Implement Transformer Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "Cvv3bN0qXX1I"
      },
      "outputs": [],
      "source": [
        "class TransEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, ff_dim, dropout, device, max_length = MAX_LENGTH):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.max_length = max_length\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        encoder_layer = TransformerEncoderLayer(hid_dim, n_heads, ff_dim, dropout, batch_first=True)\n",
        "        self.encoder = TransformerEncoder(encoder_layer, n_layers)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor([hid_dim], device = device, dtype=torch.float32))\n",
        "        \n",
        "    def forward(self, src, pos_emb, src_mask):\n",
        "        '''\n",
        "        Q3 - (c)\n",
        "        Implement forward method of TransEncoder Module\n",
        "        (Use torch.nn.TransformerEncoder, torch.nn.TransformerEncoderLayer)\n",
        "        \n",
        "        INPUT\n",
        "        - src: source language batched data (B, max_len)\n",
        "        - pos_emb: positional embedding (1, max_len, hid_dim)\n",
        "        - src_mask: padding mask tensor for source sentences (B, max_len)\n",
        "\n",
        "        OUTPUT\n",
        "        What to be returned depends on your implementation of TransSeq2Seq.\n",
        "        Feel free to return outputs you need.\n",
        "        Some examples below,\n",
        "\n",
        "        - encoder output (B, max_len, hid_dim)\n",
        "        '''\n",
        "        #################### YOUR CODE ####################\n",
        "        src = self.tok_embedding(src)\n",
        "        src = self.dropout((src*self.scale) + pos_emb)\n",
        "        \n",
        "        output = self.encoder(src, src_key_padding_mask=src_mask)\n",
        "        \n",
        "        return output\n",
        "        ###################################################\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "8mKifSxjIWKk"
      },
      "outputs": [],
      "source": [
        "class TransDecoder(nn.Module):\n",
        "    def __init__(self, out_dim, hid_dim, n_layers, n_heads, ff_dim, dropout, device, max_length = MAX_LENGTH):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.max_length = max_length\n",
        "        self.tok_embedding = nn.Embedding(out_dim, hid_dim)\n",
        "        decoder_layer = TransformerDecoderLayer(hid_dim, n_heads, ff_dim, dropout, batch_first=True)\n",
        "        self.decoder = TransformerDecoder(decoder_layer, n_layers)\n",
        "        self.fc_out = nn.Linear(hid_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor([hid_dim], device = device, dtype=torch.float32))\n",
        "        \n",
        "    def forward(self, trg, pos_emb, enc_src, trg_mask, trg_sub_mask, src_mask):\n",
        "        '''\n",
        "        Q3 - (c)\n",
        "        Implement forward method of TransDecoder Module\n",
        "        (Use torch.nn.TransformerDecoder, torch.nn.TransformerDecoderLayer)\n",
        "        \n",
        "        INPUT\n",
        "        - trg: target language batched data (B, max_len)\n",
        "        - pos_emb: positional embedding (1, max_len, hid_dim)\n",
        "        - enc_src: encoder outputs (B, max_len, hid_dim)\n",
        "        - trg_mask: padding mask tensor for target sentences (B, max_len)\n",
        "        - trg_sub_mask: subsequent mask for target sentences (max_len, max_len)\n",
        "        - src_mask: padding mask tensor for source sentences (B, max_len)\n",
        "\n",
        "        OUTPUT\n",
        "        What to be returned depends on your implementation of TransSeq2Seq.\n",
        "        Feel free to return outputs you need.\n",
        "        Some examples below,\n",
        "\n",
        "        - decoder output (B, max_len, out_dim)\n",
        "        '''\n",
        "        #################### YOUR CODE ####################\n",
        "        trg = self.tok_embedding(trg)\n",
        "        trg = self.dropout((trg*self.scale) + pos_emb)\n",
        "        dec = self.decoder(tgt=trg, tgt_key_padding_mask=trg_mask, \n",
        "                           tgt_mask=trg_sub_mask, memory=enc_src, memory_key_padding_mask=src_mask)\n",
        "        output = self.fc_out(dec).to(DEVICE)\n",
        "        return output\n",
        "        ###################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "L5VFP1-3IXmv"
      },
      "outputs": [],
      "source": [
        "class TransSeq2Seq(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hid_dim, ff_dim, n_layers, n_heads, dropout_p, device, max_length=MAX_LENGTH):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        self.hid_dim = hid_dim\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.encoder = TransEncoder(in_dim, hid_dim, n_layers[0], n_heads, ff_dim, dropout_p, device)\n",
        "        self.decoder = TransDecoder(out_dim, hid_dim, n_layers[1], n_heads, ff_dim, dropout_p, device)\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        '''\n",
        "        Q3 - (b)\n",
        "        Implement mask generating function\n",
        "        \n",
        "        INPUT\n",
        "        - src: batched input sentences (B, max_len)\n",
        "\n",
        "        OUTPUT\n",
        "        - Boolean padding mask tensor (B, max_len)\n",
        "        '''\n",
        "        #################### YOUR CODE ####################\n",
        "        source_mask = (src == dataset.input_lang_pad)\n",
        "        return source_mask.to(DEVICE)\n",
        "        ###################################################\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        '''\n",
        "        Q3 - (b)\n",
        "        Implement mask generating function\n",
        "\n",
        "        INPUT\n",
        "        - trg: batched target sentences (B, max_len)\n",
        "\n",
        "        OUTPUT\n",
        "        - A tuple of a padding mask tensor and a subsequent mask tensor ((B, max_len), (max_len, max_len))\n",
        "        '''\n",
        "        #################### YOUR CODE ####################\n",
        "        trg_mask = (trg == dataset.output_lang_pad).to(DEVICE)\n",
        "        trg_sub_mask = torch.logical_not(torch.tril(torch.ones((self.max_length, self.max_length))).bool()).to(DEVICE)\n",
        "        return trg_mask, trg_sub_mask\n",
        "        ###################################################\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        '''\n",
        "        Q3 - (c)\n",
        "        Implement forward method of TransSeq2Seq Module\n",
        "        \n",
        "        INPUT\n",
        "        - src: source language batched data (B, max_len)\n",
        "        - trg: target language batched data (B, max_len)\n",
        "\n",
        "        OUTPUT\n",
        "        - decoder output (B, out_dim, max_dim)\n",
        "        \n",
        "        '''\n",
        "        #################### YOUR CODE ####################\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask, trg_sub_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        enc_output = self.encoder(src, self.get_pos_emb(), src_mask).to(DEVICE)\n",
        "        dec_output = self.decoder(trg, self.get_pos_emb(), \n",
        "                                  enc_output, trg_mask, trg_sub_mask, src_mask).to(DEVICE)\n",
        "        dec_output = dec_output.permute(0,2,1)\n",
        "\n",
        "        return dec_output.to(DEVICE)\n",
        "        ###################################################  \n",
        "    \n",
        "    def get_pos_emb(self):\n",
        "        '''\n",
        "        Q3 - (a)\n",
        "        Implement absolute positional embedding\n",
        "\n",
        "        OUTPUT\n",
        "        - positional embedding tensor (max_len, hid_dim)\n",
        "        '''\n",
        "        #################### YOUR CODE ####################\n",
        "        pos_emb = torch.zeros(self.max_length, self.hid_dim)\n",
        "        for p in range(self.max_length):\n",
        "          for i in range(self.hid_dim):\n",
        "            if i % 2 == 0:\n",
        "              pos_emb[p,i] = math.sin(p / 10000 ** (2 * i) / self.hid_dim)\n",
        "            else:\n",
        "              pos_emb[p,i] = math.cos(p / 10000 ** (2 * i) / self.hid_dim)\n",
        "        return pos_emb.to(DEVICE)\n",
        "        ###################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqnvbugEIJ-h"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "s8XBEzSHaUdX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "97255b6a-f521-48be-e956-df100fa52a2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nQ3 - (d)\\nTrain your Seq2Seq model and plot perplexities and learning rates. \\nUpon successful training, the test perplexity should be less than 2. \\nBriefly report your hyperparmeters and results on test dataset. \\nMake sure your results are printed in your submitted file.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "'''\n",
        "Q3 - (d)\n",
        "Train your Seq2Seq model and plot perplexities and learning rates. \n",
        "Upon successful training, the test perplexity should be less than 2. \n",
        "Briefly report your hyperparmeters and results on test dataset. \n",
        "Make sure your results are printed in your submitted file.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "hdNx0Ol5ILbR"
      },
      "outputs": [],
      "source": [
        "# experiment various methods for better performance\n",
        "# you can modify the codes in this block\n",
        "in_dim = dataset.input_lang.n_words\n",
        "out_dim = dataset.output_lang.n_words\n",
        "hid_dim = 64\n",
        "ff_dim = 128\n",
        "n_enc_layers = 4\n",
        "n_dec_layers = 4\n",
        "n_layers = [n_enc_layers, n_dec_layers]\n",
        "n_heads = 8\n",
        "dropout = 0.5\n",
        "\n",
        "learning_rate=0.001\n",
        "N_EPOCHS = 30\n",
        "valid_every=2\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "trans_model = TransSeq2Seq(in_dim, out_dim, hid_dim, ff_dim, n_layers, n_heads, dropout, device).to(device)\n",
        "optimizer = torch.optim.Adam(trans_model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = dataset.output_lang_pad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "aES3_sBTIgnN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ef22f5d-e3a9-40c1-b6e7-a38ea7c728b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 4.400 | Train PPL:  81.433\n",
            "==========================\n",
            "\t Val. Loss: 2.299 |  Val. PPL:   9.968\n",
            "Epoch: 02\n",
            "\tTrain Loss: 2.203 | Train PPL:   9.055\n",
            "Epoch: 03\n",
            "\tTrain Loss: 1.560 | Train PPL:   4.760\n",
            "==========================\n",
            "\t Val. Loss: 1.063 |  Val. PPL:   2.896\n",
            "Epoch: 04\n",
            "\tTrain Loss: 1.193 | Train PPL:   3.299\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.940 | Train PPL:   2.559\n",
            "==========================\n",
            "\t Val. Loss: 0.623 |  Val. PPL:   1.865\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.745 | Train PPL:   2.107\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.601 | Train PPL:   1.823\n",
            "==========================\n",
            "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.484 | Train PPL:   1.623\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.396 | Train PPL:   1.486\n",
            "==========================\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
            "Epoch: 11\n",
            "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
            "==========================\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 12\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "Epoch: 13\n",
            "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
            "==========================\n",
            "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
            "Epoch: 14\n",
            "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
            "Epoch: 15\n",
            "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
            "==========================\n",
            "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
            "Epoch: 16\n",
            "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
            "Epoch: 17\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
            "==========================\n",
            "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
            "Epoch: 18\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "Epoch: 19\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "==========================\n",
            "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
            "Epoch: 20\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "Epoch: 21\n",
            "\tTrain Loss: 0.045 | Train PPL:   1.046\n",
            "==========================\n",
            "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
            "Epoch: 22\n",
            "\tTrain Loss: 0.039 | Train PPL:   1.040\n",
            "Epoch: 23\n",
            "\tTrain Loss: 0.034 | Train PPL:   1.034\n",
            "==========================\n",
            "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
            "Epoch: 24\n",
            "\tTrain Loss: 0.028 | Train PPL:   1.028\n",
            "Epoch: 25\n",
            "\tTrain Loss: 0.025 | Train PPL:   1.025\n",
            "==========================\n",
            "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
            "Epoch: 26\n",
            "\tTrain Loss: 0.022 | Train PPL:   1.022\n",
            "Epoch: 27\n",
            "\tTrain Loss: 0.019 | Train PPL:   1.019\n",
            "==========================\n",
            "\t Val. Loss: 0.203 |  Val. PPL:   1.224\n",
            "Epoch: 28\n",
            "\tTrain Loss: 0.017 | Train PPL:   1.017\n",
            "Epoch: 29\n",
            "\tTrain Loss: 0.015 | Train PPL:   1.015\n",
            "==========================\n",
            "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
            "Epoch: 30\n",
            "\tTrain Loss: 0.014 | Train PPL:   1.014\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1872x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Train your model\n",
        "# you can modify the codes in this block\n",
        "history = {'train_PPL':[], 'val_PPL':[], 'lr':[]}\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(trans_model, train_dataloader, optimizer, loss_fn, 1)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    \n",
        "    if epoch%valid_every==0:\n",
        "        print(\"==========================\")\n",
        "        valid_loss = evaluate(trans_model, valid_dataloader, loss_fn)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            trans_model.decoder.t=0\n",
        "            torch.save(trans_model.state_dict(), 'transformer-model.pt')\n",
        "\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "        history['train_PPL'].append(math.exp(train_loss))\n",
        "        history['val_PPL'].append(math.exp(valid_loss))\n",
        "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "4M6sUc-aJKP3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1e195d9-f4cf-4a56-9a41-a26cfe527329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Test. Loss: 0.209 |  Test. PPL:   1.233\n"
          ]
        }
      ],
      "source": [
        "# Test your model\n",
        "torch.save(trans_model.state_dict(), 'transformer-model.pt') \n",
        "loaded_model = TransSeq2Seq(in_dim, out_dim, hid_dim, ff_dim, n_layers, n_heads, dropout, device).to(device)\n",
        "loaded_model.load_state_dict(torch.load('transformer-model.pt'))\n",
        "\n",
        "test_loss = evaluate(loaded_model, test_dataloader, loss_fn)\n",
        "print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hyper Parameter]\n",
        "- LSTM의 경우와 마찬가지로, dropout은 낮은 값보다 0.5 수준에서 학습이 가장 꾸준히 진행되는 모습을 보였다.\n",
        "- learning rate 역시 0.001인 경우에 가장 performance가 좋고 학습이 잘 되었다.\n",
        "- 그 외의 hyper parameter들은 여러 경우를 돌려보며 최종적으로 결정하였다.\n",
        "\n",
        "[Test Result]\n",
        "- Train/Validation loss 모두 epoch이 지남에 따라 감소하였고, Validation loss가 증가하는 추세가 보이지 않아 overfitting이 발생했다고 보기 어렵다.\n",
        "- test perplexity는 1.233으로, 앞선 두 모델보다 낮게 나타났다.\n",
        "\n",
        "[Which Approach is Better & Why?]\n",
        "- 결론적으로 앞선 LSTM, GRU 모델보다 Transformer를 활용한 모델의 결과가 더 좋게 나타났다.\n",
        "- 앞선 두 모델들은 test perplexity가 5에 가까웠던 반면, Transformer 모델은 총 epoch이 30이었음에도 1에 근사한 test perplexity를 보여주었다.\n",
        "- 그 이유 중 첫번째는 LSTM과 GRU가 \"이전의 정보를 잘 기억하지 못한다\"는 기존 RNN의 문제점을 어느 정도 해결하였지만 완벽히 해결하지는 못했기 때문으로 볼 수 있다. 물론 본 과제는 LSTM, GRU에 Attention을 사용하여 이를 보완하였다.\n",
        "- 그 다음으로는, Transformer는 Multi-head Attention, Encoder에서의 self-attention, Decoder에서 Encoder의 hidden states 정보를 활용한 attention 등을 통해 중요한 정보에 더 많이 집중할 수 있다는 장점이 있기 때문일 것이다."
      ],
      "metadata": {
        "id": "JzR2kMpdEO6M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bhGgiMY-ITiC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}