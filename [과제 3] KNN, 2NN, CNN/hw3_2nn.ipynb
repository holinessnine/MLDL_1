{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"nCYO6dmGgefe"},"source":["# Colab Setup"]},{"cell_type":"code","metadata":{"id":"er0RD438gRLm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669469847038,"user_tz":-540,"elapsed":127613,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"c73207d8-b768-4503-f99c-4574c34f50ee"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"Jfeql_8sgnKJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669469849116,"user_tz":-540,"elapsed":419,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"037fa64a-4696-46b9-8325-81e02fe1d728"},"source":["\"\"\"\n","Change directory to where this file is located\n","\"\"\"\n","%cd '/content/drive/MyDrive/MLDL_lab/'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MLDL_lab\n"]}]},{"cell_type":"markdown","metadata":{"id":"sPEoabX-hGCh"},"source":["# Import Modules"]},{"cell_type":"code","metadata":{"id":"OyammZP8hI7P"},"source":["import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist.data_utils import load_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLxTNOvI5NHD"},"source":["#Utils"]},{"cell_type":"code","metadata":{"id":"xuQB6W2U5ZE2"},"source":["def tanh(z):\n","    \"\"\"\n","    Implement the tanh activation function.\n","    The method takes the input z and returns the output of the function.\n","\n","    Question (a)\n","\n","    \"\"\"\n","    ##### YOUR CODE #####   \n","    output = (2 / (1 + np.exp(-2*z))) - 1\n","    return output\n","    #####################\n","\n","\n","def softmax(X):\n","    \"\"\"\n","    Implement the softmax function.\n","    The method takes the input X and returns the output of the function.\n","\n","    Question (a)\n","\n","    \"\"\"\n","    ##### YOUR CODE #####\n","    logit = np.exp(X - np.amax(X, axis = 1, keepdims = True))\n","    numer = logit\n","    denom = np.sum(logit, axis=1, keepdims=True)\n","    return numer/denom\n","    #####################\n","\n","\n","def load_batch(X, Y, batch_size, shuffle=True):\n","    \"\"\"\n","    Generates batches with the remainder dropped.\n","\n","    Do NOT modify this function\n","    \"\"\"\n","    if shuffle:\n","        permutation = np.random.permutation(X.shape[0])\n","        X = X[permutation, :]\n","        Y = Y[permutation, :]\n","    num_steps = int(X.shape[0])//batch_size\n","    step = 0\n","    while step<num_steps:\n","        X_batch = X[batch_size*step:batch_size*(step+1)]\n","        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n","        step+=1\n","        yield X_batch, Y_batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tsU8v_6khR30"},"source":["#2-Layer Neural Network"]},{"cell_type":"code","metadata":{"id":"mA5udiGmhRb5"},"source":["class TwoLayerNN:\n","    \"\"\" a neural network with 2 layers \"\"\"\n","\n","    def __init__(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        Do NOT modify this function.\n","        \"\"\"\n","        self.input_dim = input_dim\n","        self.num_hiddens = num_hiddens\n","        self.num_classes = num_classes\n","        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n","\n","    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        initializes parameters with Xavier Initialization.\n","\n","        Question (b)\n","        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization \n","        \n","        Inputs\n","        - input_dim\n","        - num_hiddens\n","        - num_classes\n","        Returns\n","        - params: a dictionary with the initialized parameters.\n","        \"\"\"\n","        params = {}\n","        ##### YOUR CODE #####\n","        params[\"W1\"] = np.random.uniform(-1 / np.sqrt(input_dim), 1 / np.sqrt(input_dim), (input_dim, num_hiddens))\n","        params[\"b1\"] = np.zeros((1, num_hiddens))\n","        params[\"W2\"] = np.random.uniform(-1 / np.sqrt(num_hiddens), 1 / np.sqrt(num_hiddens), (num_hiddens, num_classes))\n","        params[\"b2\"] = np.zeros((1, num_classes))\n","        #####################\n","        return params\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Define and perform the feed forward step of a two-layer neural network.\n","        Specifically, the network structue is given by\n","\n","          y = softmax(tanh(X W1 + b1) W2 + b2)\n","\n","        where X is the input matrix of shape (N, D), y is the class distribution matrix\n","        of shape (N, C), N is the number of examples (either the entire dataset or\n","        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n","\n","        Question (c)\n","        - ff_dict will be used to run backpropagation in backward method.\n","\n","        Inputs\n","        - X: the input matrix of shape (N, D)\n","\n","        Returns\n","        - y: the output of the model\n","        - ff_dict: a dictionary with all the fully connected units and activations.\n","        \"\"\"\n","        ff_dict = {}\n","        ##### YOUR CODE #####\n","        params = self.params\n","\n","        ff_dict['s1'] = np.matmul(X, params['W1'])\n","        ff_dict['s2'] = ff_dict['s1'] + params['b1']\n","        ff_dict['s3'] = tanh(ff_dict['s2'])\n","        ff_dict['s4'] = np.matmul(ff_dict['s3'], params['W2'])\n","        ff_dict['s5'] = ff_dict['s4']  + params['b2']\n","        y = softmax(ff_dict['s5'])\n","        #####################\n","        return y, ff_dict\n","\n","    def backward(self, X, Y, ff_dict):\n","        \"\"\"\n","        Performs backpropagation over the two-layer neural network, and returns\n","        a dictionary of gradients of all model parameters.\n","\n","        Question (d)\n","\n","        Inputs:\n","         - X: the input matrix of shape (B, D), where B is the number of examples\n","              in a mini-batch, D is the feature dimensionality.\n","         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n","              where B is the number of examples in a mini-batch, C is the number\n","              of classes.\n","         - ff_dict: the dictionary containing all the fully connected units and\n","              activations.\n","\n","        Returns:\n","         - grads: a dictionary containing the gradients of corresponding weights and biases.\n","        \"\"\"\n","        grads = {}\n","        ##### YOUR CODE #####\n","        dl_ds = self.forward(X)[0] - Y\n","\n","        ds_1 = np.matmul(dl_ds, self.params['W2'].T) * (1 - np.power(ff_dict['s3'], 2))\n","        grads[\"dW1\"] = np.matmul(X.T, ds_1)\n","        grads[\"db1\"] = np.mean(ds_1, axis=0, keepdims=True)\n","        grads[\"dW2\"] = np.matmul(ff_dict['s3'].T, dl_ds)\n","        grads[\"db2\"] = np.mean(dl_ds, axis=0, keepdims=True)\n","        #####################\n","        return grads\n","\n","\n","    def compute_loss(self, Y, Y_hat):\n","        \"\"\"\n","        Computes cross entropy loss.\n","\n","        Do NOT modify this function.\n","\n","        Inputs\n","            Y:\n","            Y_hat:\n","        Returns\n","            loss:\n","        \"\"\"\n","        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n","        return loss\n","\n","    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n","        \"\"\"\n","        Runs mini-batch gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X\n","        - Y\n","        - X_val\n","        - Y_Val\n","        - lr\n","        - n_epochs\n","        - batch_size\n","        - log_interval\n","        \"\"\"\n","        for epoch in range(n_epochs):\n","            for X_batch, Y_batch in load_batch(X, Y, batch_size):\n","                self.train_step(X_batch, Y_batch, batch_size, lr)\n","            if epoch % log_interval==0:\n","                Y_hat, ff_dict = self.forward(X)\n","                train_loss = self.compute_loss(Y, Y_hat)\n","                train_acc = self.evaluate(Y, Y_hat)\n","                Y_hat, ff_dict = self.forward(X_val)\n","                valid_loss = self.compute_loss(Y_val, Y_hat)\n","                valid_acc = self.evaluate(Y_val, Y_hat)\n","                print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n","                      format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n","\n","    def train_step(self, X_batch, Y_batch, batch_size, lr):\n","        \"\"\"\n","        Updates the parameters using gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X_batch\n","        - Y_batch\n","        - batch_size\n","        - lr\n","        \"\"\"\n","        _, ff_dict = self.forward(X_batch)\n","        grads = self.backward(X_batch, Y_batch, ff_dict)\n","        self.params[\"W1\"] -= lr * grads[\"dW1\"]/batch_size\n","        self.params[\"b1\"] -= lr * grads[\"db1\"]/batch_size\n","        self.params[\"W2\"] -= lr * grads[\"dW2\"]/batch_size\n","        self.params[\"b2\"] -= lr * grads[\"db2\"]/batch_size\n","\n","    def evaluate(self, Y, Y_hat):\n","        \"\"\"\n","        Computes classification accuracy.\n","        \n","        Do NOT modify this function\n","\n","        Inputs\n","        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n","             where C is the number of classes.\n","        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n","             where C is the number of classes.\n","\n","        Returns\n","            accuracy: the classification accuracy in float\n","        \"\"\"        \n","        classes_pred = np.argmax(Y_hat, axis=1)\n","        classes_gt = np.argmax(Y, axis=1)\n","        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n","        return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XXM2lWhtDYC6"},"source":["#Load MNIST"]},{"cell_type":"code","metadata":{"id":"48ooR6YIxYhC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669469863744,"user_tz":-540,"elapsed":4971,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"8d7e7f61-f6c0-42bb-a5a1-2a5478db367b"},"source":["X_train, Y_train, X_test, Y_test = load_data()\n","\n","idxs = np.arange(len(X_train))\n","np.random.shuffle(idxs)\n","split_idx = int(np.ceil(len(idxs)*0.8))\n","X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n","X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n","print()\n","print('Set validation data aside')\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', Y_train.shape)\n","print('Validation data shape: ', X_valid.shape)\n","print('Validation labels shape: ', Y_valid.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MNIST data loaded:\n","Training data shape: (60000, 784)\n","Training labels shape: (60000, 10)\n","Test data shape: (10000, 784)\n","Test labels shape: (10000, 10)\n","\n","Set validation data aside\n","Training data shape:  (48000, 784)\n","Training labels shape:  (48000, 10)\n","Validation data shape:  (12000, 784)\n","Validation labels shape:  (12000, 10)\n"]}]},{"cell_type":"markdown","metadata":{"id":"tzw-D4Zr5xoi"},"source":["#Training & Evaluation"]},{"cell_type":"code","metadata":{"id":"IlnC_rerHPaN"},"source":["### \n","# Question (e)\n","# Tune the hyperparameters with validation data, \n","# and print the results by running the lines below.\n","###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTCqVT4S0Tm5"},"source":["# model instantiation\n","model = TwoLayerNN(input_dim=784, num_hiddens=128, num_classes=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train the model\n","lr, n_epochs, batch_size = 1.0, 15, 64\n","model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7x3q48-I2RFQ","executionInfo":{"status":"ok","timestamp":1669469913208,"user_tz":-540,"elapsed":48199,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"b8c88867-f681-4b4b-b0d4-5d86a1edab05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 00 - train loss/acc: 0.378 0.906, valid loss/acc: 0.412 0.902\n","epoch 01 - train loss/acc: 0.178 0.950, valid loss/acc: 0.244 0.942\n","epoch 02 - train loss/acc: 0.123 0.963, valid loss/acc: 0.193 0.951\n","epoch 03 - train loss/acc: 0.145 0.954, valid loss/acc: 0.217 0.941\n","epoch 04 - train loss/acc: 0.089 0.972, valid loss/acc: 0.176 0.958\n","epoch 05 - train loss/acc: 0.081 0.974, valid loss/acc: 0.172 0.957\n","epoch 06 - train loss/acc: 0.070 0.978, valid loss/acc: 0.164 0.961\n","epoch 07 - train loss/acc: 0.077 0.974, valid loss/acc: 0.180 0.956\n","epoch 08 - train loss/acc: 0.034 0.990, valid loss/acc: 0.144 0.967\n","epoch 09 - train loss/acc: 0.047 0.985, valid loss/acc: 0.157 0.963\n","epoch 10 - train loss/acc: 0.037 0.988, valid loss/acc: 0.166 0.963\n","epoch 11 - train loss/acc: 0.026 0.991, valid loss/acc: 0.146 0.967\n","epoch 12 - train loss/acc: 0.020 0.994, valid loss/acc: 0.144 0.968\n","epoch 13 - train loss/acc: 0.024 0.992, valid loss/acc: 0.151 0.965\n","epoch 14 - train loss/acc: 0.019 0.994, valid loss/acc: 0.154 0.966\n"]}]},{"cell_type":"code","metadata":{"id":"hpPsAlXU0T_Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669469913212,"user_tz":-540,"elapsed":98,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"ccb8c3c4-238f-4df2-dfbc-3471f975c3bc"},"source":["# evalute the model on test data\n","Y_hat, _ = model.forward(X_test)\n","test_loss = model.compute_loss(Y_test, Y_hat)\n","test_acc = model.evaluate(Y_test, Y_hat)\n","print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Final test loss = 0.141, acc = 0.967\n"]}]},{"cell_type":"markdown","metadata":{"id":"5hh1PZpk_g0I"},"source":["# Extra Credit (Optional)"]},{"cell_type":"code","metadata":{"id":"0R0n6y9_AgXc"},"source":["def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n","    \"\"\"\n","    initializes parameters with He Initialization.\n","\n","    Question (f)\n","    - refer to https://paperswithcode.com/method/he-initialization for He initialization \n","    \n","    Inputs\n","    - input_dim\n","    - num_hiddens\n","    - num_classes\n","    Returns\n","    - params: a dictionary with the initialized parameters.\n","    \"\"\"\n","\n","    params = {}\n","    ##### YOUR CODE #####\n","    params['W1'] = np.random.randn(input_dim, num_hiddens) * (np.sqrt(2 / input_dim))\n","    params['b1'] = np.zeros((1, num_hiddens))\n","    params['W2'] = np.random.randn(num_hiddens, num_classes) * (np.sqrt(2 / num_hiddens))\n","    params['b2'] = np.zeros((1, num_classes))\n","    #####################\n","    return params\n","\n","def forward_relu(self, X):\n","    \"\"\"\n","    Defines and performs the feed forward step of a two-layer neural network.\n","    Specifically, the network structue is given by\n","\n","        y = softmax(relu(X W1 + b1) W2 + b2)\n","\n","    where X is the input matrix of shape (N, D), y is the class distribution matrix\n","    of shape (N, C), N is the number of examples (either the entire dataset or\n","    a mini-batch), D is the feature dimensionality, and C is the number of classes.\n","\n","    Question (f)\n","\n","    Inputs\n","        X: the input matrix of shape (N, D)\n","\n","    Returns\n","        y: the output of the model\n","        ff_dict: a dictionary containing all the fully connected units and activations.\n","    \"\"\"\n","\n","    ff_dict = {}        \n","    ##### YOUR CODE #####\n","    ff_dict['s1'] = np.matmul(X, self.params['W1'])\n","    ff_dict['s2'] = ff_dict['s1'] + self.params['b1']\n","    ff_dict['s3'] = np.maximum(ff_dict['s2'], 0)\n","    ff_dict['s4'] = np.matmul(ff_dict['s3'], self.params['W2'])\n","    ff_dict['s5'] = ff_dict['s4'] + self.params['b2']\n","    y = softmax(ff_dict['s5'])\n","    #####################\n","    return y, ff_dict\n","\n","def backward_relu(self, X, Y, ff_dict):\n","    \"\"\"\n","    Performs backpropagation over the two-layer neural network, and returns\n","    a dictionary of gradients of all model parameters.\n","\n","    Question (f)\n","\n","    Inputs:\n","        - X: the input matrix of shape (B, D), where B is the number of examples\n","            in a mini-batch, D is the feature dimensionality.\n","        - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n","            where B is the number of examples in a mini-batch, C is the number\n","            of classes.\n","        - ff_dict: the dictionary containing all the fully connected units and\n","            activations.\n","\n","    Returns:\n","        - grads: a dictionary containing the gradients of corresponding weights\n","            and biases.\n","    \"\"\"\n","\n","    grads = {}\n","    ##### YOUR CODE #####\n","    dim = X.shape[1]\n","    dl_ds = self.forward(X)[0] - Y\n","\n","    ds_1 = np.matmul(dl_ds, self.params['W2'].T) * np.greater(ff_dict['s3'],0).astype(float)\n","    grads['dW1'] = np.matmul(X.T, ds_1)\n","    grads['db1'] = (1/dim) * np.sum(ds_1, axis=0, keepdims=True)\n","    grads['dW2'] = np.matmul(ff_dict['s3'].T, dl_ds)\n","    grads['db2'] = (1/dim) * np.sum(dl_ds, axis=0, keepdims=True)\n","    #####################\n","    return grads\n","\n","TwoLayerNNRelu = copy.copy(TwoLayerNN)\n","TwoLayerNNRelu.initialize_parameters = initialize_parameters\n","TwoLayerNNRelu.forward = forward_relu\n","TwoLayerNNRelu.backward = backward_relu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qY3T98XvEIP"},"source":["### \n","# Question (f)\n","# Tune the hyperparameters with validation data,\n","# and print the results by running the lines below.\n","###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-jJRXqsBxzh"},"source":["# model instantiation\n","model_relu = TwoLayerNNRelu(input_dim=784, num_hiddens=64, num_classes=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EC8f80a0w53m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669470359427,"user_tz":-540,"elapsed":31477,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"0494c620-4b66-4c2b-8e80-3754ffe40bf5"},"source":["# train the model\n","lr, n_epochs, batch_size = 0.5, 20, 128\n","history = model_relu.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 00 - train loss/acc: 0.186 0.944, valid loss/acc: 0.206 0.939\n","epoch 01 - train loss/acc: 0.120 0.964, valid loss/acc: 0.148 0.956\n","epoch 02 - train loss/acc: 0.085 0.976, valid loss/acc: 0.122 0.964\n","epoch 03 - train loss/acc: 0.068 0.980, valid loss/acc: 0.112 0.967\n","epoch 04 - train loss/acc: 0.065 0.980, valid loss/acc: 0.113 0.966\n","epoch 05 - train loss/acc: 0.058 0.983, valid loss/acc: 0.113 0.966\n","epoch 06 - train loss/acc: 0.048 0.986, valid loss/acc: 0.106 0.968\n","epoch 07 - train loss/acc: 0.048 0.985, valid loss/acc: 0.113 0.966\n","epoch 08 - train loss/acc: 0.037 0.989, valid loss/acc: 0.108 0.970\n","epoch 09 - train loss/acc: 0.028 0.993, valid loss/acc: 0.098 0.972\n","epoch 10 - train loss/acc: 0.044 0.986, valid loss/acc: 0.118 0.969\n","epoch 11 - train loss/acc: 0.025 0.993, valid loss/acc: 0.103 0.971\n","epoch 12 - train loss/acc: 0.020 0.995, valid loss/acc: 0.101 0.974\n","epoch 13 - train loss/acc: 0.020 0.995, valid loss/acc: 0.103 0.972\n","epoch 14 - train loss/acc: 0.017 0.996, valid loss/acc: 0.103 0.973\n","epoch 15 - train loss/acc: 0.013 0.998, valid loss/acc: 0.099 0.973\n","epoch 16 - train loss/acc: 0.012 0.998, valid loss/acc: 0.105 0.972\n","epoch 17 - train loss/acc: 0.011 0.998, valid loss/acc: 0.104 0.972\n","epoch 18 - train loss/acc: 0.015 0.996, valid loss/acc: 0.114 0.971\n","epoch 19 - train loss/acc: 0.010 0.999, valid loss/acc: 0.108 0.973\n"]}]},{"cell_type":"code","metadata":{"id":"4i__6TfpCqOc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669470359431,"user_tz":-540,"elapsed":86,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"4973438c-c54d-4fde-b0c1-22ac66e20875"},"source":["Y_hat, _ = model_relu.forward(X_test)\n","test_loss = model_relu.compute_loss(Y_test, Y_hat)\n","test_acc = model_relu.evaluate(Y_test, Y_hat)\n","print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Final test loss = 0.088, acc = 0.977\n"]}]},{"cell_type":"markdown","source":["- Discussed with: 강창훈(데이터사이언스학과)\n","- Reference: lab1 (for question a)"],"metadata":{"id":"lnLTdl2hqPYA"}}]}