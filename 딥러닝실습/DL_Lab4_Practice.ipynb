{"cells":[{"cell_type":"markdown","metadata":{"id":"EDxtMC2g66gQ"},"source":["# Colab Setup"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"vj2CXov7JJqq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670810798588,"user_tz":-540,"elapsed":8530,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"63749825-3c14-4062-e489-a15706fa787e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"DcKp4bZiJwut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670810810131,"user_tz":-540,"elapsed":1148,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"b054ec3b-5bdd-41db-f4fd-54c1007d1b0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MLDL_lab\n"]}],"source":["\"\"\"\n","Change directory to where this file is located\n","\"\"\"\n","%cd '/content/drive/MyDrive/MLDL_lab/'"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"MpJDwdhDZqE8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670769373258,"user_tz":-540,"elapsed":10409,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"385a38cb-c8ae-41e0-d88d-b2cdc0cc0f38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 35.1 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 54.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 71.5 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"]}]},{"cell_type":"code","execution_count":40,"metadata":{"id":"UHKo6dP6eEO6","executionInfo":{"status":"ok","timestamp":1670810812296,"user_tz":-540,"elapsed":2,"user":{"displayName":"신성구","userId":"07958575584326347145"}}},"outputs":[],"source":["import math\n","import pickle\n","from pathlib import Path\n","import sys\n","import random\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torchtext\n","\n","import pandas as pd\n","import numpy as np\n","\n","from data.data import prepareData"]},{"cell_type":"code","source":["\"\"\"\n","import modules you need\n","\"\"\""],"metadata":{"id":"1oHn0919bMSm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","print(\"Using PyTorch version: {}, Device: {}\".format(torch.__version__, DEVICE))\n","print(\"Using torchtext version: {}\".format(torchtext.__version__))"],"metadata":{"id":"rdvF2PE0Zzjh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670810816400,"user_tz":-540,"elapsed":791,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"6d59ec41-d24a-4a5f-ccb0-15a8daef7fb9"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Using PyTorch version: 1.13.0+cu116, Device: cuda\n","Using torchtext version: 0.14.0\n"]}]},{"cell_type":"code","execution_count":42,"metadata":{"id":"OOnSsL1EeG85","executionInfo":{"status":"ok","timestamp":1670810817191,"user_tz":-540,"elapsed":1,"user":{"displayName":"신성구","userId":"07958575584326347145"}}},"outputs":[],"source":["SEED = 1111\n","\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","source":["# Practice 1. Seq2seq model with Attention Mechanism"],"metadata":{"id":"2CU-kWCgakT9"}},{"cell_type":"markdown","metadata":{"id":"5PgCsIyawXoN"},"source":["## Question 1.  Dataset"]},{"cell_type":"markdown","source":["[About the Dataset]\n","* Translation Task\n","  - French(src) -> English(trg)\n","  - Sequence to Sequence\n","* Language tokens\n","  - src: 4,345 words in our dictionary\n","  - trg: 2,803 words in our dictionary\n","* Set max length to 10\n","* 10,599 pairs"],"metadata":{"id":"sU5Tn4er79Sk"}},{"cell_type":"code","execution_count":43,"metadata":{"id":"4Lvt4lGJIe8i","executionInfo":{"status":"ok","timestamp":1670810902632,"user_tz":-540,"elapsed":847,"user":{"displayName":"신성구","userId":"07958575584326347145"}}},"outputs":[],"source":["MAX_LENGTH = 10\n","BATCH_SIZE = 64\n","\n","TRAIN_RATIO = 0.7  # train dataset ratio, should be a float in (0, 0.8]\n","VALID_RATIO = 0.8 - TRAIN_RATIO\n","\n","SOS_token = 0\n","EOS_token = 1"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"fePBsU2GKoaI","executionInfo":{"status":"ok","timestamp":1670810903432,"user_tz":-540,"elapsed":2,"user":{"displayName":"신성구","userId":"07958575584326347145"}}},"outputs":[],"source":["class TranslateDataset(Dataset):\n","    def __init__(self, max_length=10, fra2eng=True):\n","        self.input_lang, self.output_lang, self.pairs = prepareData('eng', 'fra', max_length=max_length, reverse=fra2eng)\n","        self.max_length=max_length\n","\n","        self.input_lang.addWord('PAD')\n","        self.output_lang.addWord('PAD')\n","        self.input_lang_pad = self.input_lang.word2index['PAD']\n","        self.output_lang_pad = self.output_lang.word2index['PAD']\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","        pair = self.pairs[idx]\n","        x, y = self._tensorsFromPair(pair)\n","        return x, y\n","\n","    def _tensorFromSentence(self, lang, sentence):\n","        indexes = [lang.word2index[word] for word in sentence.split(' ')]\n","        indexes.append(EOS_token)\n","        return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n","\n","    def _tensorsFromPair(self, pair):\n","        input_tensor = self._tensorFromSentence(self.input_lang, pair[0])\n","        target_tensor = self._tensorFromSentence(self.output_lang, pair[1])\n","        return (input_tensor, target_tensor)\n","    \n","    def collate_fn(self, data):\n","        x_batch = []; y_batch = []\n","        \n","        for x, y in data:\n","            if x.shape[0] < self.max_length-1:\n","                x = torch.cat([x, self.input_lang_pad*torch.ones((self.max_length-1 - x.shape[0], 1), dtype=x.dtype)])\n","            elif x.shape[0] > self.max_length-1:\n","                x = x[:self.max_length-1]\n","            if y.shape[0] < self.max_length-1:\n","                y = torch.cat([y, self.output_lang_pad*torch.ones((self.max_length-1 - y.shape[0], 1), dtype=y.dtype)])\n","            elif y.shape[0] > self.max_length-1:\n","                y = y[:self.max_length-1]\n","\n","            x_batch.append(torch.cat([torch.tensor([SOS_token]), x.squeeze(1)]))\n","            y_batch.append(torch.cat([torch.tensor([SOS_token]), y.squeeze(1)]))\n","        \n","        return torch.stack(x_batch), torch.stack(y_batch)"]},{"cell_type":"code","source":["dataset = TranslateDataset(max_length=MAX_LENGTH)\n","        \n","print(\"\\n\")\n","print(\"This is data example\")\n","print(random.choice(dataset.pairs))\n","\n","train_size = int(len(dataset)*TRAIN_RATIO)\n","valid_size = int(len(dataset)*VALID_RATIO)\n","train_data, valid_data, test_data = random_split(dataset, [train_size, valid_size, len(dataset)-(train_size+valid_size)],)\n","print(\"\\n\")\n","print(f\"This is dataset_size: {len(dataset)}\")\n","print(f\"train_size: {train_size}\")\n","print(f\"valid_data: {valid_size}\")\n","print(f\"test_data: {len(test_data)}\")\n","\n","train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)\n","valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)"],"metadata":{"id":"K1m8QPaPhifP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670810910631,"user_tz":-540,"elapsed":3401,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"46f894e1-18e8-47d4-f423-841bba7ed7fd"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading lines...\n","Read 135842 sentence pairs\n","Trimmed to 10599 sentence pairs\n","Counting words...\n","Counted words:\n","fra 4345\n","eng 2803\n","\n","\n","This is data example\n","['vous etes une de ces menteuses !', 'you are such a liar !']\n","\n","\n","This is dataset_size: 10599\n","train_size: 7419\n","valid_data: 1059\n","test_data: 2121\n"]}]},{"cell_type":"code","source":["\"\"\"\n","(1) Print the 1st elements of the 1st batch in the train_dataloader.\n","\n","(2) Then, convert the elements into word format.\n","You can use dataset.input_lang.index2word and dataset.output_lang.index2word to convert the indices into words.\n","Any printed format is allowed if you can check the results.\n","\"\"\"\n","########## Your Code #########\n","x, y = next(iter(train_dataloader)) # 따로 만들어줌\n","sample_x = x[0]\n","sample_y = y[0]\n","sample_x_sentence = f\"{' '.join([dataset.input_lang.index2word[i] for i in sample_x.tolist()])}\"\n","sample_y_sentence = f\"{' '.join([dataset.output_lang.index2word[i] for i in sample_y.tolist()])}\"\n","\n","print(\"sample_x: \", sample_x) ## (1)\n","print(sample_x_sentence) ## (2)\n","print(\"\\nsample_y: \", sample_y) ## (1)\n","print(sample_y_sentence) ## (2)\n","##############################"],"metadata":{"id":"z6lIHD3Xf6Ik","colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"status":"ok","timestamp":1670800879554,"user_tz":-540,"elapsed":925,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"d985231a-5a76-49a1-826f-3c51736ebaef"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_x:  tensor([   0,   89,   24, 1595,  202, 1108,    5,    1, 4345, 4345])\n","SOS maintenant il regarde la tele . EOS PAD PAD\n","\n","sample_y:  tensor([   0,   14,   40,  662,  595,  238,    4,    1, 2803, 2803])\n","SOS he is watching tv now . EOS PAD PAD\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n<강의영상 코드>\\nsample_x, sample_y = next(iter(Train_dataloader))\\nsample_x = sample_x.squeeze(0)\\nsample_y = sample_y.squeeze(0)\\n\\nprint(\"sample_x: \", sample_x)\\nprint(\\' \\'.join([dataset.input_lang.index2word[i] for i in sample_x.tolist()]))\\nprint(\"sample_y: \", sample_y)\\nprint(\\' \\'.join([dataset.output_lang.index2word[i] for i in sample_y.tolist()]))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["\n","```\n","[answer example for Q(1) & Q(2)] \n","sample_x:  tensor([   0,  210,  211,  121, 2640,    5,    1, 4345, 4345, 4345])\n","SOS tu es tres efficace . EOS PAD PAD PAD\n","\n","sample_y:  tensor([   0,  129,   78,  303, 1548,    4,    1, 2803, 2803, 2803])\n","SOS you re very efficient . EOS PAD PAD PAD\n","```\n","\n"],"metadata":{"id":"yvW0PNKNFuzq"}},{"cell_type":"code","source":["\"\"\"\n","(3) Print the words whose indices are 0 and 1 in each dataset of input_lang and output_lang.\n","Check whether the indices 0 and 1 represent the same words in the input_lang and the output_lang.\n","You can use dataset.input_lang.index2word and dataset.output_lang.index2word to convert the indices into words.\n","\"\"\"\n","########## Your Code #########\n","input_lang_zero_word = dataset.input_lang.index2word[0]\n","output_lang_zero_word = dataset.output_lang.index2word[0]\n","input_lang_one_word = dataset.input_lang.index2word[1]\n","output_lang_one_word = dataset.output_lang.index2word[1]\n","\n","print(\"The index 0 in each language represents....\")\n","print(f\"fra: {input_lang_zero_word}\")\n","print(f\"eng: {output_lang_zero_word}\")\n","print(\"\\nThe index 1 in each language represents....\")\n","print(f\"fra: {input_lang_one_word}\")\n","print(f\"eng: {output_lang_one_word}\")\n","##############################\n"],"metadata":{"id":"nYZxh6NUbqYb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670800995651,"user_tz":-540,"elapsed":11,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"eef76fa9-0ccd-4dc7-9545-ac39c60ebccd"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["The index 0 in each language represents....\n","fra: SOS\n","eng: SOS\n","\n","The index 1 in each language represents....\n","fra: EOS\n","eng: EOS\n"]}]},{"cell_type":"code","source":["\"\"\"\n","(4) Print the index of [Pad] in each dataset of input_lang and output_lang.\n","Check whether the input_lang and the output_lang have same index for [PAD].\n","\"\"\"\n","########## Your Code #########\n","input_lang_pad_idx = dataset.input_lang.word2index['PAD']\n","output_lang_pad_idx = dataset.output_lang.word2index['PAD']\n","\n","print(\"Check the indices of PAD token for each language\")\n","print(f\"fra: {input_lang_pad_idx}\")\n","print(f\"eng: {output_lang_pad_idx}\")\n","##############################"],"metadata":{"id":"gkYyDIVxe1dV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670801077661,"user_tz":-540,"elapsed":11,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"365e4f9e-3272-4431-b6ac-28b1e14b9ebf"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Check the indices of PAD token for each language\n","fra: 4345\n","eng: 2803\n"]}]},{"cell_type":"markdown","metadata":{"id":"aFDRFkQTC3YQ"},"source":["## Question 2. Encoder"]},{"cell_type":"markdown","source":["We are going to implement the Encoder part using single layer of LSTM."],"metadata":{"id":"xMyL16Mk8_h8"}},{"cell_type":"code","source":["# Initialize Model\n","in_dim = dataset.input_lang.n_words # number of words in french (=4346)\n","out_dim = dataset.output_lang.n_words # number of word in english (=2804)\n","emb_dim = 512 # embbeding size\n","hid_dim = 256 # vector size of encoder output\n","\n","print(f'\\nin_dim: {in_dim}\\tout_dim: {out_dim}\\temb_dim: {emb_dim}\\thid_dim: {hid_dim}\\n')"],"metadata":{"id":"vyfff3iTi1iJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670801117355,"user_tz":-540,"elapsed":447,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"601f0bf3-c9b6-47f6-d9f7-c91fc1549e82"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","in_dim: 4346\tout_dim: 2804\temb_dim: 512\thid_dim: 256\n","\n"]}]},{"cell_type":"code","source":["\"\"\"\n","(1) Try embedding the sample_x using the in_dim and emb_dim (use nn.Embedding)\n","Print the shapes of the embeded_x. embeded_x will be used again in Q(3).\n","\"\"\"\n","x, y = next(iter(train_dataloader))\n","sample_x = x[0]\n","print(f\"sample_x: {sample_x.shape}\")\n","\n","########## Your Code #########\n","embedded_x = nn.Embedding(in_dim, emb_dim)(sample_x)\n","print(f\"embedded_x: {embedded_x.shape}\")\n","##############################"],"metadata":{"id":"EbSJB9B-jl-4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670801119426,"user_tz":-540,"elapsed":3,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"a404548e-a72c-4955-9436-2f37e28b1b73"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_x: torch.Size([10])\n","embedded_x: torch.Size([10, 512])\n"]}]},{"cell_type":"code","source":["\"\"\"\n","(2) Initialize hidden and cell states for unbatched input.\n","You can initialize the states using torch.zeros.\n","Use hid_dim for their intialized sizes; Hout, Hcell.\n","Initialized hidden and cell states will be used again in Q(3).\n","Ref: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n","\n","- cell_0 : (1, hid_dim)\n","- hidden_0 : (1, hid_dim)\n","\"\"\"\n","\n","########## Your Code #########\n","hidden_0 = torch.zeros(1, hid_dim)\n","cell_0 = torch.zeros(1, hid_dim)\n","print('Initialize hidden and cell states')\n","print(f'hidden_0: {hidden_0.shape}\\tcell_0: {cell_0.shape}')\n","##############################"],"metadata":{"id":"MEH1RRbe1DrA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670801185390,"user_tz":-540,"elapsed":447,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"07b996d4-b25b-4ba6-c648-9408c1d06c46"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Initialize hidden and cell states\n","hidden_0: torch.Size([1, 256])\tcell_0: torch.Size([1, 256])\n"]}]},{"cell_type":"code","source":["\"\"\"\n","(3) Get outputs of Encoder using torch.nn.LSTM.\n","Use embedded_x, hidden_0, cell_0 as inputs. \n","enc_hiddens will be used again in Decoder-Q(3) and Attention-Q(2)\n","Ref: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n","\n","INPUT\n","- embedded_x : (MAX_LENGTH, emb_dim)\n","- hidden_0 : (1, hid_dim)\n","- cell_0 : (1, hid_dim)\n","\n","OUTPUT\n","- hiddens : (MAX_LENGTH, hid_dim)\n","- hidden : (1, hid_dim)\n","- cell : (1, hid_dim)\n","\"\"\"\n","\n","emb_dim = 512 # embbeding size\n","hid_dim = 256 # vector size of encoder output\n","\n","########## Your Code #########\n","lstm = torch.nn.LSTM(input_size=emb_dim, hidden_size=hid_dim)\n","\n","hiddens, (hidden, cell) = lstm(embedded_x, (hidden_0, cell_0))\n","\n","print('LSTM Encoder outputs')\n","print(f'hiddens: {hiddens.shape}\\thidden: {hidden.shape}\\tcell: {cell.shape}')\n","##############################\n","\n","######### DO NOT MODIFY ########\n","print('\\n')\n","check = (\"Yes\" if torch.sum(hiddens[-1]-hidden) == 0 else \"No\")\n","print(f\"Is hidden equal to the last value of hiddens? : {check}\")\n","print(f\"Assigning encoder outputs for decoder (w/attention, see next question)...\")\n","enc_hiddens = hiddens\n","################################"],"metadata":{"id":"XveBtVbhkMWp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670801191746,"user_tz":-540,"elapsed":447,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"057f6aa2-9a6e-4a18-ac17-66988fe03bda"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["LSTM Encoder outputs\n","hiddens: torch.Size([10, 256])\thidden: torch.Size([1, 256])\tcell: torch.Size([1, 256])\n","\n","\n","Is hidden equal to the last value of hiddens? : Yes\n","Assigning encoder outputs for decoder (w/attention, see next question)...\n"]}]},{"cell_type":"markdown","metadata":{"id":"m3jkpgL7C7-_"},"source":["## Question 3. Decoder"]},{"cell_type":"markdown","source":["We are going to implement the Decoder part using single layer of LSTM."],"metadata":{"id":"HXzRN8-a9S0K"}},{"cell_type":"code","source":["\"\"\"\n","(1) Try embedding the sample_y using the given out_dim and emb_dim (Use nn.Embedding)\n","Print the shapes of the embedded_y.\n","embedded_y will be used again in Q(3).\n","\"\"\"\n","\n","########## Your Code #########\n","embedded_y = nn.Embedding(out_dim, emb_dim)(sample_y)\n","print(f\"sample_y: {sample_y.shape}\")\n","print(f\"embedded_y: {embedded_y.shape}\")\n","##############################"],"metadata":{"id":"kdho8e70mV6h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670801337505,"user_tz":-540,"elapsed":3,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"51039f04-1862-4629-bb5c-88aebfe9bb99"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_y: torch.Size([10])\n","embedded_y: torch.Size([10, 512])\n"]}]},{"cell_type":"code","source":["\"\"\"\n","(2) Initialize the inputs (hidden_0, cell_0, input_0) for the Decoder\n","They will be used again in Q(3).\n","Please refer to the class material of Lecture 9 for more details.\n","Ref: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n","\n","- cell_0 : Initialize cell state for unbatched input. \n","           Use hid_dim for its intialized size; Hcell. (1, hid_dim)\n","           You can initialize the cell state using torch.zeros.\n","- input_0 : (embed_dim)\n","- hidden_0 : (1, hid_dim)\n","             You have to use the output of the Encoder for hidden_0.\n","\"\"\"\n","\n","########## Your Code #########\n","cell_0 = torch.zeros(1, hid_dim)\n","input_0 = embedded_y[0]\n","hidden_0 = hiddens[-1].unsqueeze(0)\n","print('LSTM Decoder inputs')\n","print(f'input_0: {input_0.shape}\\thidden_0: {hidden_0.shape}\\tcell_0: {cell_0.shape}')\n","##############################"],"metadata":{"id":"FHABukOMpY6u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670806944885,"user_tz":-540,"elapsed":456,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"75debdc6-9fdf-4ed7-83ce-3989d7a6d337"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["LSTM Decoder inputs\n","input_0: torch.Size([512])\thidden_0: torch.Size([1, 256])\tcell_0: torch.Size([1, 256])\n"]}]},{"cell_type":"code","source":["\"\"\"\n","(3) Starting with the hidden_0, cell_0, input_0, apply the Decoder uing torch.nn.LSTM.\n","You have to compute 'seq2seq_outputs_hiddens' and 'seq2seq_outputs_index' in this question.\n","\n","- 'seq2seq_outputs_index': contains indices of the predicted words\n","- 'seq2seq_outputs_hiddens': stacked tensors of decoded hiddens (MAX_LENGTH, 1, hid_dim)\n","                          It will be used again in Attention-Q(2)\n","\n","Refer to the class material of Lecture 9 for details.\n","Ref: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n","\"\"\"\n","\n","seq2seq_outputs_hiddens = []\n","seq2seq_outputs_index = [0]  # 0: [SOS]\n","\n","decoder = nn.LSTM(input_size=emb_dim, hidden_size=hid_dim)\n","dec_embedder = nn.Embedding(out_dim, emb_dim)\n","classifier = nn.Linear(hid_dim, out_dim)\n","fc = nn.Linear(hid_dim + hid_dim, hid_dim)\n","tanh = nn.Tanh()\n","\n","hidden = hidden_0\n","cell = cell_0\n","input = input_0.unsqueeze(0)\n","\n","for t in range(MAX_LENGTH):\n","    hidden_encout_concat = torch.cat([hidden, enc_hiddens[-1].unsqueeze(0)], dim=1) # (1, 256)\n","    hidden = fc(hidden_encout_concat)\n","    hidden = tanh(hidden)\n","\n","    ########## Your Code #########\n","    hiddens, (hidden, cell) = decoder(input, (hidden, cell)) # output 내기\n","    next_token = F.softmax(classifier(hidden), dim=1).argmax() # hid -> out dim으로\n","    seq2seq_outputs_index.append(next_token.item()) # next_token은 텐서 -> item 뽑자\n","    seq2seq_outputs_hiddens.append(hiddens)\n","\n","    input = dec_embedder(next_token).unsqueeze(0)\n","    ##############################\n","\n","print(f'seq2seq_outputs_index : {seq2seq_outputs_index}') # predicted sentence (length 10, )\n","seq2seq_outputs_words = ' '.join([dataset.output_lang.index2word[i] for i in seq2seq_outputs_index])\n","print('\\t'+seq2seq_outputs_words)\n","seq2seq_outputs_hiddens = torch.stack(seq2seq_outputs_hiddens)\n","print(f'\\nseq2seq_outputs_hiddens : {seq2seq_outputs_hiddens.shape}')"],"metadata":{"id":"l81Du2A-20uI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","```\n","[answer example]\n","seq2seq_outputs_index : [0, 1375, 618, 889, 2241, 1632, 978, 789, 1482, 705, 1425]\n","\tSOS shot embarrassed myself educated grumbling fishmonger counting near arrogant german\n","seq2seq_outputs_hiddens : torch.Size([10, 1, 256])\n","```\n","\n"],"metadata":{"id":"ehgYhF4gJHmR"}},{"cell_type":"markdown","source":["## Question 4. Attention "],"metadata":{"id":"-bvNZig21awh"}},{"cell_type":"code","source":["\"\"\"\n","(1) Please apply attention mechanism to the given query, key and value.\n","You have to compute attention score, attention coefficient and attention value.\n","\"\"\"\n","query = torch.rand(1, 256)\n","key = torch.rand(10, 256)\n","value = torch.rand(10, 128)\n","\n","########## Your Code #########\n","attn_score = torch.matmul(key, query.permute(1,0)) # 10,1\n","print(attn_score.shape)\n","attn_coefficient = F.softmax(attn_score, dim=0) # 10,1\n","print(attn_coefficient.shape) \n","attn_value = torch.sum(value * attn_coefficient, dim=0) # sum - dim=0 -> col별로\n","print(attn_value.shape)\n","##############################"],"metadata":{"id":"T7kekYJ91boa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670804368872,"user_tz":-540,"elapsed":4,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"93da58f3-0584-48ad-a5ae-9bf395650c15"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 1])\n","torch.Size([10, 1])\n","torch.Size([128])\n"]}]},{"cell_type":"code","source":["\"\"\"\n","(2) Please apply attention mechanism to the given query, key and value (key = value).\n","You have to compute attention score, attention coefficient and attention value. \n","\"\"\"\n","# key = value\n","kv = enc_hiddens\n","print(\"Key/value shape:\\t\",  kv.shape)\n","\n","# query\n","example_t = 4 # any int [0 ~ MAX_LENGTH-1]\n","q = seq2seq_outputs_hiddens[example_t]\n","print(\"Query shape:\\t\",  q.shape)\n","\n","########## Your Code #########\n","attn_score = torch.matmul(kv, q.T)\n","attn_coefficient = F.softmax(attn_score, dim=0)\n","weighted_kv = kv*attn_coefficient\n","weighted_sum = torch.sum(weighted_kv, dim=0)\n","##############################\n","\n","print(\"attn_score shape:\\t\",  attn_score.shape)\n","print(\"weights: \\t\", attn_coefficient.squeeze(1).tolist())\n","print(\"total of weights: \\t\", sum(attn_coefficient.squeeze(1).tolist()))\n","print(\"weighted sum of val:\\t\", weighted_kv.shape)\n","print(\"weighted sum:\\t\", weighted_sum.shape)"],"metadata":{"id":"lSsUqg_8x4y-","colab":{"base_uri":"https://localhost:8080/","height":264},"executionInfo":{"status":"error","timestamp":1670804589978,"user_tz":-540,"elapsed":15,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"5d849549-0f44-40cb-b79e-9672816a8903"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Key/value shape:\t torch.Size([10, 256])\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-7e0de9e4a81e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mexample_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;31m# any int [0 ~ MAX_LENGTH-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq_outputs_hiddens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Query shape:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'seq2seq_outputs_hiddens' is not defined"]}]},{"cell_type":"code","source":["sample_idx = 3  # which index of encoder output to attend [0~9]\n","\n","print(\"before attn\")\n","print(sum(kv[sample_idx]))\n","print(\"attn weight\")\n","print(attn_coefficient[sample_idx])\n","print(\"after attn\")\n","print(sum(weighted_kv[sample_idx]))"],"metadata":{"id":"Mh495Oi1zwyP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Practice 2. Seq2seq model with Transformer"],"metadata":{"id":"siPUPEFXEbrD"}},{"cell_type":"code","source":["# initialize model\n","in_dim = dataset.input_lang.n_words\n","out_dim = dataset.output_lang.n_words\n","emb_dim = 256\n","MAX_LENGTH = 10"],"metadata":{"id":"F5yc8iJ9GFrJ","executionInfo":{"status":"ok","timestamp":1670804632627,"user_tz":-540,"elapsed":2,"user":{"displayName":"신성구","userId":"07958575584326347145"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# embedding input\n","embedded_x = nn.Embedding(in_dim, emb_dim)(sample_x)\n","print(\"embedded_x:\\t\", embedded_x.shape)"],"metadata":{"id":"SZw7whv9GSdY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670804634657,"user_tz":-540,"elapsed":5,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"2d6d7a84-736c-4169-d4c4-9e83875ab457"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["embedded_x:\t torch.Size([10, 256])\n"]}]},{"cell_type":"code","source":["\"\"\"\n","(1) Positional Encoding\n","\n","(1)-a : Make the Learnable Embedding (using nn.Embedding) and Visualize it using matplotlib.pyplot.pcolormesh.\n","\n","(1)-b : Absolute sinusoid-baesd Positinal Encoding is given in the file of pe_fall22.pickle.\n","Visualize it using matplotlib.pyplot.pcolormesh.\n","\n","Please refer to the images sinusoid.png & learnable.png in data directory as answer exmples.\n","\"\"\"\n","import matplotlib.pyplot as plt\n","\n","########## Your Code #########\n","# a. Learnable Embedding (using nn.Embedding)\n","pos = torch.arange(MAX_LENGTH).unsqueeze(1)\n","pos_embedding = nn.Embedding(in_dim, emb_dim)(pos)\n","plt.pcolormesh(pos_embedding.squeeze(1).detach().numpy(), cmap='RdBu')\n","plt.show()\n","\n","# b. Absolute sinusoid-baesd Positinal Encoding\n","with open('./data/pe_fall22.pickle', 'rb') as handle:\n","  pe = pickle.load(handle)\n","plt.pcolormesh(pe.squeeze(1), cmap='RdBu')\n","\n","plt.show()\n","\n","\n","\n","\n","##############################\n","\n","\n"],"metadata":{"id":"xIg5RLUzEXQc","colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"status":"ok","timestamp":1670805904096,"user_tz":-540,"elapsed":777,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"5e680567-2177-4431-e068-671602c44603"},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZf7//+c9LTOZ9EIKIQRI6L2LiICoWLGgKzYsiA3LujZEV11777JiL4gNERQVASnSJXQINZSEJKT3ZCYzc3//uKPrb3/LLgtRP2d9P64rF1PPudt55cxkhrfSWiOEEMK6bL93A4QQQhwbCXIhhLA4CXIhhLA4CXIhhLA4CXIhhLA4CXIhhLC4/xjkSqm3lFLFSqktv7gtTik1Xym1q/nf2F+3mUIIIQ7nSM7I3wFG/9NtdwMLtdZZwMLm60IIIX4H6ki+EKSUygC+0lp3b76+AxiutS5USqUAi7XWnX7NhgohhPjXHEf5vCStdWHz5SIg6XAPVEpNBCYCeN2ufp06dSJUWUKNtxUAntI8nBHh2DwRaKfbPCkUAF8Dyun6aSOEHB5QzVe1xh8C10+vJ0IBsDlQTQ3U7c8HILxDB1CKpoI8AGyp6QSCGldtqdmGzQbhUahQwGzCFY5qqEa7I6GiCICmBh9hiUmUBswwRbsdBEMadegAAK7UdJS/AW1vHsaGWoiIAx0kiB2AoAZXQyV+TwwA/mAIr9OGaqwFoNrmIdJXgYqIM9sINoG/EV9JGe7UFAC008PW/Go6BEt+Hld3QjxBr9lmU1BjUwrnT32LiEGFAtQRRrijedDqKsDtRddWAlDsiMXrsuOpNNOoQ5pAQmvsyjzeZgN7KABao0JBAOoP5OOMcBMKmOuu1m1R/jpQpq/aZoeGamhuV7C0CHtia6r9QcKd5jGOkB9sDgLNLwadgUb8dje/PJ1waT+hqnIA7N5ImqoqcISHgyfC7Ke2ChURQ9DmNI9RGoIBguXFzcshiC05HXvI37xFBf4Gc8lhnlMecuEtzcfZuq3pb6AR7Wug4VCZWT/paYQcHqr9Zn3EKD9NDg8NTSEimxeeri5BRcZD81xW2LzE2fzU28w6Dg/5zJjUVwOwoz6MDq0icAR9FPvNNlqpBmrsEbgd5npDIEi08rO/3sxDur0eHC5CbtN3VVGE8nhpckWa8SNA06ECqmNT8TRvI9xhM+uoeV0GSwuxxyfTGDLbdKsgKhig0P+PF+QxbiduO+iKQ2Y/Hi+hulpsceY4VcEmNArd3JfG0irsbidOr8fsw+fHV9WAt0N7GvbtNfMYFU6oKYAOhQAIS0rBbwvD3rxbmw6igk2EHO6fZgl//l6KS+pJ69PV3Bbw47eFoUrNca0T0nBqPzSZuW10erEfysPVykRQY8FB3CkplAScxPvNGrJ5ItB2J2jTDhUKop1hVDYvjxgXBEoKsdnNGrXFJKB0CN18LAD4cBJm06gms460w02gtJD6GHOMRrnsEAriO2iyQadm4CZAzc5cItu1Nhtxe6nwhYiqLmoesybsaRnYg80NsTsBRVOoeXzKC7A5zRyuzz1YqrVO5DCO9oy8Umsd84v7K7TW//F98r6Z6Xr1ymXUz5nG94NuBKDn638mZWhfwnocRzCli+lAbQmhfZtxJGeYQbG78CV15qdxVaEgebUh0r3N1xuq0J5onIVb+fH6yQD0njkTbHYKH7oZgKh7p1JUFyBj+TSzD48Xe7/R2GvMwvWl9ca5dSENXUbCZ08AULppD21vuIW3Ss34ndkpgfKGIJ6nTdtTH3gZR/4mdGQCAIGtK2DoRdh8NVTZowCo8gVps3UOB7qeDUBedSMDUyNw71gCwHxPH4bv/gz7CX8y7ao5RGj/Vna/9j6d778PgKbkLnS/cwGzKt/4eSyzrrmYun7nApBf00S400brlW8DYB9yHvbaUtbZ2tIjwfwyVKtmYu80AP+KLwF4pdV5DEyLoeucR8w+6hqpuPoJIppDKsJlI6qhGEJBbA1VAKy/8Q5Sh3SkscxcT330TZx7V6OaQyYYHktww0Jsg0xfK998jKgbHmXRgVp6J5vHJNbnE4xIpDwUBkBSeQ550Z0JNq/DkIYMfz71380AwDvgREq//oKYvn2xdx0CgH/llziHnk+lxxy8UcqPvbaE8ukvA9BYVo1n8ivE1uY1LxAbOj/HjE1cMgCf1LZm4Dt3kPjImwCEF+8gkLuJLc+9b9bPS0/hS+rMgr3mF9+ZYQcoTujGxqI6TmxjFp7+bhr2EZehti8DYKZ7ABd689jk6QxAT99uQp5oAusXADB8XRazJh1HYu0+Xtxrwusm91a+jxpM54RwALaV1HGqcz/XZZt5eyluPfb4FOo6nmjWx8wn8HQbQFE7cz0pWE7RCw8y/5z76Zpoxrh3YhiOqgKCkSaEa995mMhL72BXg9lnR3s59tpSHtsb9fN6OqtLEp2iwP/5cwB4ug2gNns54ReY48deVYR2OGla+51p55tfE9cpmaQBJnBr9hewZ+5G+s/6iK1XXQFA+qhe1BeV46+pAyDj9ikc9LYjOswEpsdfhb2qiPpW5sW8kxB5fxnPS9OyeapqAwCOsn0cjOyA87W7zBq79gmSGw/CIfPLYkfyELzP30SbSbcDsPOvU+g05R5eK01m/H6zhsJ6DSUUk4oKNJrlUFtOMCmLL/NNYp6VZqPi9UcIizG/HL1nX42tqR5tc4Ayx8NOWwodwgM48jeZdiR3pOy1R9hw7l8BGJEega2xir13TwLA98DbdLGVsnjEhYz40BxjwczBzMqtY+T8JwGo3ldI7GPvEF1j1mkwKhmUjaJGE3QRMx7Ek2Syx3vhXdla6/4cxtF+auVQ81sqNP9bfJTbEUIIcYyONsjnAOObL48HZrdMc4QQQvy3juTjhzOAlUAnpVS+Uupq4HHgZKXULmBU83UhhBC/g//4x06t9bjD3HVSC7dFCCHEUZBvdgohhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMVJkAshhMUp3Vy9/LfQr3N7/d2k84k+b4KpUA2oUICmNV9z8PgJXPraagAW3HY8AE4dAMBemY92eqj3mqrpZQ0BUtfO4OA33wOQ8Ojb2BS4t85HZw4EYEZuE/HhLk5JNRW7K/EQ6y+jPtxUpQ7fMo9AyUHCOvYGIFBWxKmrE5l3aXt0mKlGbq86iHZ5+WHUhQBUTP+CMaEtbJj8BAAbHniL8e0UKn+bub/DMOIqdhHYuwV7J9MO7QiD3HXYvKZCN5EJ+DcuQZ88EYArP97Ee+nb2ZR5FgAZM/9G3NirCOxaT1Gv8wBYur+SszvF42koA8BRVYC2u9Bhppr7wupoes9+iNVnTgFgVLsYwoq20bRrPcrhNPsdOIYD9TbSt8wy497nFIKeGKbM2w3AUz39qKDfVPIGSt98iqbqenbO2cLQtUvNGGmIO/5Gts9/AYBkrxN3wSZC4bEAhHLX40hsTUVKHwBiynex2dGWrvu+Q7lMBXebNwodk4J2esz4uMIJLf+UUIOptL6q13iG1WRTt85Upt/9xWp6v/gEwz+r4pVL+gLQYclLOBJb4x9yEQBNQc2HWw5xTbdoABo/fRb7uCm4y3NNuzctJfvRDxk4azoFtjgA2lRu42t/W5K8plp95wQ3+68ZS8bpg8xc5uzHeceLON41VdJjzruKz8tjiXTZGRXXYNbU9BfYdvYUEpu3Ef/eFLae91eGuU0tclVZhAqPZldERwDauRqx52+maOYnVE58CoCOB77nrqJMnhhs5vL57UGSzz+Li5dONfPmiUJXFrMxYTAAsR47aU4faucKAD519GXxrlKuGdwWt9Ocl3UpXk2otpLdHUYD4Hp4AukPv2zWIhC0h+HwVUPIHF+2vesJ1lRS3fdcHNP/ZnZ7xf3Yaw5x6BVTAT7xzmdxlO+jbsGnZhuNfvYvWE/7M814bXlnMf1nfYSj/ABlrXqacQ9pYpa+SePBAgAixt6Adrp5b6cZv3NWvUj9pQ/y3W6zri/pmcSBmy9m282vcGbDjwDsTBtO58B+VjSlmmXcykGJ307ZdRcA0PXlqbB3A1vThpu+75iD6nMK9soCgkV7zTw4nAS7nUTFC3eadtzyNKEZjxBx/KkA1KT1w730XSo2bgEg/Oan8VYdgJL9hNJNX2y52eye+jqZ119j1mXaCbSLUKw55AfAFwzRvVU4seu/ACBUXYZtxOXMz/fxcXYeAPef2okEj51qfwiA1sXrCVYUs+3paWYtXDicy+pG8nyuuV5303PU+IKm323jsrXW/TkMOSMXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLkyAXQgiLO6YgV0r9WSm1VSm1RSk1QynlbqmGCSGEODJHHeRKqdbAzUB/rXV3wA5c1FINE0IIcWSO9a0VB+BRSjmAcKDg2JskhBDiv3FMxZeVUrcAjwANwHda60v+xWMmAhMB4pJT+22+8Wzmn3YPY3e+C8CGV+bR797LcPQaQVFYCgBx81/CmdGFT12m1mhWvJeO858h/IwrAbBVFlCW0peoldMBCJ14OTvGnU36qF4w/kEAYir3UPTWy8T16gyAY8i5LDzuXDqe3QWANvc8zqoaLwNz5wBwQ2lv/j4ylsrpLxB33hUABA/kUNTtTApqTIHV9p8+SNyIU/jE1guAszvF496xBJ3aybSrJJdgVRmhuhp0fbUZgNHXc8pLK3n4vB4A9F3+Mp4egwl2NAWmd1SFyFrzFq4+IwHIe/VZwu95Fc/sp3g3azwAE3on4yjfR15YGgApaz5gX79L2N3HbGNs6+OpnvdXfG5TBLmuKURZfZDMnC/Ibn8GAOFOO503zvh5XmyRsWxseyrdW5kiyDW+ILENRex/eLIZn8dfN+2rhk7bZwMwP+VUJj2zmB+fNYWi44vWU7XwS6JOMy/EQof2savtSNouehGAyp15zDrlbsKddq6IMwWJA/EZBH/4GF/BQQAqzptM6obPcKRlmvvzd5OdeTZfbC4C4LZhGcS4bNhrS9gTigGg4dqxRKREkXaq6X/N7n1ET7z/54LOoa9foWbPfgINZt5yLn6YfguewTvuNtZWmmLc9U0hvtxSxMM1MwHwnPQnCl5+kvAUU5x595gp9PXWQ44pAk3PUThKc/lm+AQyTzNtbXveaPL7jaOweX0cV5PNJ6GujG00RcRV5gBCYRHYaw4B0JQ9H33yRML2ruLDRrONcXojK254hKRepuh1WGwk3ilTCYTMcdmq4Ec2R/emZNRJAHRZvoSkphL0zuZ9dByErbGatRNu5eELHgXg/Yt74a07xEcHTeHtrPhw7ErRK9oU8rUf3ELBh++TcuPdADRGp1H+0A1E3/sqESU7APD9+B2l63JIudUUnw554yh/6R7sNzwJQPSPn2DrPozQ9lUA3FfTm4djNhPqdxbOfaZwcjC5I+xYyTRM0exr6hbzVdKpnNXGFF7f63eTVbuTXn834/PjgyPZWuqj46LnCZ5nCiWHB2ppcEYS9sP7Zt0OPgfV1Iitscqsl5g2hJxuapsLGkfZmnAcWM/bte1YsrMEgHcHB5jd2JYzk5pMXzYvpnHQBby/yayx68J3QaCJ0szhACSW70C7PPhXzSVsoCnQfNDbjlRfIaHd2WbMdm8j4oTT+esuU/D7ukFtCAFJGz4HoLTP+ZTfcAExHVJoNfl5s261nfDGcmy+GtMOTzTvdBhJqsfM0+gvH0eFuVngNFkxdN00FvaaAMCYbim/TvFlpVQsMAZoB6QCXqXUpf/8OK31NK11f611/4iY+KPdnRBCiMM4lrdWRgF7tdYlWusm4HNgSMs0SwghxJE6liA/AAxWSoUrpRRwEpDTMs0SQghxpI46yLXWq4HPgHXA5uZtTWuhdgkhhDhCjmN5stb6fuD+FmqLEEKIoyDf7BRCCIuTIBdCCIuTIBdCCIuTIBdCCIuTIBdCCIuTIBdCCIuTIBdCCIuTIBdCCIuTIBdCCIuTIBdCCIuTIBdCCIuTIBdCCIuTIBdCCIuTIBdCCIuTIBdCCIuTIBdCCItTWuvfbGc9UxL09HV7cNoUW3oeB8Bpz49j43HXEx/u/Plx6wtruMC5C3/uVgAcA89A+euo+PQNAOwTHiHw2mQSzr0EgGBUMnkqljbBUub1ORuAU2Y+wIHMU8jIXw5A47Y1fNX9ai6wbwegqPVg4ldP58eO5wMwsJUD36fPUD3mTlrlfA1A04GdVO8rpPrqxwBot+tb1j8wlXaffQVAtCNEaOHbVJ14NQBxm76kau1qDoy9n0SvqdmREizHXlVAjrcrAO1/fIeSoVcReGii2cfkv9M6wsk7G01F72vbgwo0snPynZTc/zYAvZLCiTq0hWxnFgB9nKUUhaXQaqtpBz1Gou0uGpQLgNKGINNWHeDRvjZCXlPw+sMOJzJgy2qy3PUATPymgEsHtGFEkxnjUF01+zNGsDLPVCc/uUMsaw7W0PHZ68iaYiqpr9Bt6bt6Ku6hY0zbNy+j+Ic1pJx3LgCBHqfAt1OZf/2bAHTfvBLP63ez/PnFnPeDKR619ub76P/iQ/jS+wFQ/8Z9xJwxjrKZ75ox7d0bR1ZfqKsAwN9+MLYVn7C49WhGhRWYth7aR3HHkylrCAAQ/ug1tL3sYkINdQBkZ4wmM9ZNjDbXdze66RgqRJXnE8hoLkSuNayeRc7UjwDo8u4n6KUf4uxmys6GwiIof+95Im96CgA1fxphXQcSqqmkbP5cAGIGDmZLhzM4WNMIwOjWDmy1JTQs/BgAT4/BrIweQHKEmZf2DbmgNYPePsS8xAUARF14IyFPDGhTBT7kCsdRXURoy1IACnqeS1qoDB0WAYA9byNN+XvY8dYsANwvf0zGti+xR8awOGogAP0WPEPE+dfx6i6zzfJaP+P7tebx7/cAcP2Hd2Jz2el0hTlWPkg4nVPn/I3UCTehw7xmfIr2ULF4HpHXPQrA8Y8u4cfxMfjSepv7v5uGbcTl1GH6VvvojdTe8gK1V5xDxQtmTE9s40UFfLDWrNPq/mPxzn8VZbfzE1e/UdR8MwOAiH7HE+gynLKnbiPidlN5fk3/oYz89EkKUwcBkFK3F3/2AmwjxwMQdIbjqtiPKs838+SJZGdEZ1K+epLynP1mqu9+lTRHA9vrwwDIWvMWrt4joL4SgJLWA7ArRdTazwBo3LsTV2IrXL1OJJDQHoAdVSEyV75O6VpzvLQefxWr3V1x2sy5cK/oIMVB98/9WpZXxYWRhdzQYSx3lWw2/Xv/r8SNvYpFjckATP0hl6nlH6LsZhtLR93B8enRxNrNurbXFLPuymsBGDRvUbbWuj+HIWfkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcRLkQghhcccU5EqpGKXUZ0qp7UqpHKXUcS3VMCGEEEfGcYzPfwH4Vms9VinlAsJboE1CCCH+C0cd5EqpaGAYcAWA1toP+FumWUIIIY7Usby10g4oAd5WSq1XSr2hlPL+84OUUhOVUmuVUmvL6xuPYXdCCCH+FaW1PronKtUfWAUcr7VerZR6AajWWt93uOdkucL1gsJiWjfsJ5RnqtnfcjCTx0/riNYajzYn9Pv+chWZN9/I+avM74WZo9zMGnQJZ7xuKkrnDriCTkUreLS0HQCTe4Xx6q4QJ2bEkR5lqnq7HTbUvKnoYBCAqpOuJ2bxNLCZCt6bX/2C7AfeInHMGQCM2vMjkQfWoKMSWe5LAqBjvIdTHl3Mk1eZ4tWjqlZCRi9UwQ4A6tYtI/fLNex72FSAP6tpI7VZw/CsnUVpr3NMO6Y/QOwp57Dc0RmAgblzONT3Alqt+RCAvNnfEfHYu0S6zO9U+4LXcfY9iYLwttT4TBX0zr5cqC0jWGOqfpd3Hc3aLoM58fHzAfBV1lB84f102DLTzE3/Mzhw7yTa33YnTYmZ5jZfLdUfPod7wsMAhD5+jJ2fLid9xhwAEkq30pTchRKfAuBQXYDeNRuoXbkAz3mTAJi2vYGJXSOxVxea/SZksafSR8b85wD4sscEzs+MYO1pZwHQ9ZLjmdnveka2j6NtXS4A/qROOMv2UvHJNABihpzI2snPkXFydwCaquuJmvIKweZl6bApiuqaSPA4qPKZucyo3EqoqowJO1IAuOqFSRz/2WuE3NGmb2ER2OvKsDVUAbA9vCNZeYtZETeErnMeASDyukeZ3roPV655B4C1k+6hy+dzqWsyYx636UscrTPx794IwII2ZzJ3axEvdy7F1+F4ANaffDL9v/kS23ZT8d4en0p9cjfKG007tYbEcAfZhXUADIrxUWmPpq4pRJznHy+GGwIhnDYz7tUPXsu3Yx9iYusaAAI71rKl07ksO1AOwLV9U7BXFdDwjVlzntOvJBCdQmVjEH/zoCll9h3RvKZiynZSMed9Ys6fAMC+x+7HHR9N4ojhAKxqcyqD8+bhyOhG8OBOAMqWLqEi5wCZ115m2lF0APvJV/HDIdO3CJedtEgXKXV7Aaj5ZgYf97qWq105HJozC4D665+mMRgiwmna0caXT8HLT/LMwNsAuD37eQ4s3knbOd+Y8bMpor55Ds+AUdQs+QqAwpVbWHD1c1zww7MARE96nMYPHsFz8V0ABOe+iu3sW7Ft+NpsIzUTFfDRuG4xrsyeACyN7EdGjJvUNR+YNTb8Cmr9IcobTF8yIzVq4zwaept1G15bhLY7UaEAoU2LALBFx+PfvYmqPQcBKM85QHiraFo98iYAy/Nq6J8aQWSTOUaDK2bx1AXPcfGB9SR5zVx7i7ZSv/JrnOf82Tzm29cI6zeKQHwGAPn3Xsd34x7nrI4JACQFStE7VgEQNmxctta6P4dxLGfk+UC+1np18/XPgL7HsD0hhBBH4aiDXGtdBOQppTo133QSsK1FWiWEEOKIHeunVm4Cpjd/YiUXuPLYmySEEOK/cUxBrrXeABz2fRshhBC/PvlmpxBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJwEuRBCWJzSWv9mO+vfo4v+ZNEaUr0OSh4xldlTrr8dVXWIUFwbKme8DEBUrz4MXpjImlu7ARDyRBO0h6FMoXH2XH0+mZecie24cwFYVmZnWNUaQo11EDJV0He8+h5tRvQmcugp5kmR8awOptJ35xcA1B03jujcZQSyjv+5fU3YcFcXsItEANpH2Wn84BEiTx4LgH/jEuxJ6fhztwKgz70Dd10JwfXzAdjf+wLi3A6K6wKkRJjiS9vPHE3/Ryfxzpl/BeDily+mobiCmsv+BkDyoql83308oxvXm310Hk5DIMSFb2fzweWmlnV0mJ2ZOaUcqKgHYHJGJZcuV7wZtwaAWa3HMLZxNfbYVgAEK4ohFKRm/Y9E9hsMwMLYoQxb/zruIaZS+HcnXs6Wd2dSVd8EwKUzJ9P+6Wkon6n4vskfyzOLdvH8mK6s6WK2kb52GXv6DmHM0mkADJxeyzOX9yPCZfqaEO6gtb+QrysiATjTnU8gPoPdDS4yol1mkL96EXev4ymb+xkAceeOZ3tYezpXrAOgduUCis66k7brPgLgGedwbj74Ic6EJAoGXgpAmr0Oti3Fnt4FgMCeTTQOOI/d5T4AuscqVMBHGV4AznpuGR9NGkJ0mJ1Ih1nv13yew+RRHelYuAKAgk8+JvnOx2DbUgAOdjqNwlo/rhsuBCDi7Vm0jXZhCzZRNfVeABacdAedEiLYVlILwJ98q7G53OjWnc16Wj6LgkVraCgz9yf0bEd0x3a42nf/ec0FMwebau0rPwegdODFBDXsqWgA4ISy5TRsW0f4aFPNPpS7nobeZ/HTcdsQ0MSHqrDlbSFUXQ5AzcZsIi+6hQ/2mWNhcFoMqREOmkLN+9SaKl+QxXsrAJiQVMH2sPa0/f557JExAIR1H8KhuC6EmuNh58iRnPDVO8yviTNzEO2mC8U0zJ8OwPbht2BTiqwFz1K4cgsAi69+jqsyQoS85jmOsn2c9Hkl33bbDcDKu6YRP/c7PA9dDUD4o+/idSpW5tcwcMHTZty79iBw6ACOM24A4NBjfyE8JQ7n1Q+b+/euYPP9T5L6lhm/qJXTcaZ1QDc1EewwEAC/w0NFY5CkTbPNGFaXYR86ltCar8z1qjKaautoGDvZbOO7V3CeeCHflLo5I7IEAK1sTFjqY/Jis9/op94netE0vs28CICBraNouO9KojJSzDZufJxqf4i42gPY6sy85MX3IjX3e2hjci0YnYqzKIeyme8CEHfyGdStXUpdYRkA7luewfujOU7cJ43P1lofthqbnJELIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFHXOQK6XsSqn1SqmvWqJBQggh/jstcUZ+C5DTAtsRQghxFI4pyJVSacAZwBst0xwhhBD/rWMqvqyU+gx4DIgEbtdan/kvHjMRmAiQnpTQb+WkscQPHoizfQ8AtN2Jr1UnKp68hbJrngSgfYyL/Jom2nkCANh8tWRNXsnGF84GIKe0gZ5r3/q5iG2osQ6by41KSEOX5gPwia0Xf4oppimpk9lG9pc07t7GrlG3AdA1+21sETE425lCqNoRhnaFE9i8FEdSGwAas05AzX6Gb7tfBcCYuCpURQE7n3kBgI4PPESupz11TUEAvs8tY1KXMNixAkdKewD8O9dh73sKvogkAM58bc4ANecAABU4SURBVA3fZG3hwJyFAGTeejPbY/uS5WkEwF5VSKh4P4FDedgHj2nufw2NsRnsKPtFceGmBvxh0QAsz6sh89WbSXv0dTPoOgRrv2Jf17NI+OQhAEovvI/M0nU/F452DDidJdURDEkzhZLdhVsIHNyNI9W0uyC2K/bX7iLxiltRflP0uSGxIxpwLDD72TdrAb5qH91eeRWAH2qjyHrnbpJvud+0IxQksPpL7PHJFHY+HYBFHQcw5uExeHsPAKBy+VIiO3Zg5wffANDpnc+wb55P/qczzTbufpU4j4OyhgBpO8xjanqeSWzhemb7TVvHhB+k9PP3mXWCmdvxPRJwVBXgjzHzuLvCR/f6HLaEdyHObQcgOW8Frzd05JKct8x4RETgP20STa9PAWDtaXczMtVJaOkMAIIjryJs63wKO4yk7i8XA6Ae/4B2Xs2OatPUMIei6vJzSPzAFPk9UOVjc3ENfVOiAOgc7yHw2mS8rROpOPVWAJJ2LSDU/ST08k/Mmtu3h40n3cYJah8AgfgMHGX7WHyOKT7s++RLRqa5qX79ATN+O/NoPWowe4ZcQ0evWYfOohxyInsQ5zF9jacO1VjDD7WmHX3mP0P4JXdi32OKd+d/NIO0iy+BhDasDZh1Gv3AFUQ9O4OIzx8DYN3L8xnyzI00Db7A7CPkJ7TgLfbNXgRAROtEkm57COwugmERZjwOZFOY2JsdZWb99Pz6cbwTH8G2wvS1al02MVdPpulbcx7oHnw6y0LpDN7/NcGyIgAOrd7M9glP03HqLQC0u+lW8uO6k+orBMBWX0HVd58Tcf51ZrzWzMXRfzT1c98m7GIzl7a1s1n/t9fI+nwuAMvyqjkt0Y+tJBeA8nmzmdbzeiYOSDNjetflRD/+LtGOEBUBc64bp+sgFEC7TEHvhg+foPFPU4hdb4q564HnsCSvjn7zngAg5sRT0UkdUIW72JdmCry3r86hMK4bRbWm4Hmsx079LRfR5fHHzRqLTELbHNRPNzk4//ibOS/OFMh2tun+6xRfVkqdCRRrrbP/3eO01tO01v211v0TYqOOdndCCCEO41jeWjkeOFsptQ/4CBiplPqgRVolhBDiiB11kGutJ2ut07TWGcBFwPda60tbrGVCCCGOiHyOXAghLM7REhvRWi8GFrfEtoQQQvx35IxcCCEsToJcCCEsToJcCCEsToJcCCEsToJcCCEsToJcCCEsToJcCCEsToJcCCEsToJcCCEsToJcCCEsToJcCCEsToJcCCEsToJcCCEsToJcCCEsToJcCCEsToJcCCEsTmmtf7Od9e/eWa/+6iOK3p1K8p9MVbiTvgkxr08+df3OpcYfAiA1UALAkmpTjbvdyzeRcc01NB3YCcBLFz5H+5WLObVDLABhm+cR6joc++5VfOvuA8CpsbXYGmuYU2uqgoc77Zxsz6UutRcA7tpD5ARi6RRtfpc1aDv+oCahLIfSz98HIOaKO3hhayN/jsn9uQ/BsiJmxZ8EgC8QYlT7OLYNGQZAYNbXjIr3UfHuMySce4m5LS4de1UR/uTOADh3LKVxyypqDhwCIH7YiTiS0mnK3wOAPTIGlZAGdRUcaNXPtOPTR3BGhbNm0PXmMUoB0HnmgwC4Ir3sOP1O6s8aDUDmGd1Qdhvhd71EbOF6APy7N7Gkw/mcmB4JwIqDtZTWN7Gm31AALjq5Hb0fug0dMBW+i7/9htqDJZTvLmPnU6YU66j2cbTavZD9GSPMvJRkEywrwhZutmmLjqfoo3eJH3IcAMrp4vHGPkzuFUbpm08B0OriifhWfU35NjOmKRdcRCixPbevqAXg2U5lNLUbRGjO82ae+o2gOLEHCfUFhJweM+7h8Ry8eRwdrrkCgFDX4TgObibYqgMA5fZotgwZTv/VSwDYM24M7mkz6Vz2I0QmAFAWk4ln9lO4zv8LAPd9f4CzuiUzOLwSgPq5b+Pp2gd7cjsA9kdm8cH6Aq4ZkEbCjvlmHhJSCVWWYIs1a6x2yRy8A05k4pY4AP4+MpZdoTjaR5lq9nrBm2zpfRmdFj6H58wJZkHt28DzI27nppKNAMzdXcHQbx4jb/E2AHpPe5GGuPa4a0xV+ZA3Dm1zoL9/x8zTceM5WOOjlddFu9pdAOzwZJKx5BX2DLsBgE77FqA69EMFzdy+1fUsrv7hFQLFBwFwJKdT23YQtf4QXqdZW+GBWtAh5vc8xayp7GUkffE4TXUNAER2644rsxcNq78FIKxzf2zRCZTNnk5EBzNmhUOuJKNyK5/UtQHgvHQ7vrBogiGTOd7aQuz1FSy98CYA2p/cmcBfXiRl2RvseN9s94M5u3ikYj3Bua+a/Qw8lWB0MisrXAAMSvXS8M6DRJ12EQBP7fVycmYivRwl+GPMfhsDIaJ2LqKg3YkAJNsbCYZFMLetOb7OzpmPvboYGmvMunV52PX4o6SffgKMNsecq2g7gdxNzE4YBcC5ST4CK7/g0MoNAKTdei8HXSn47zVrcsuNL9H+b1cR2yGZ+J5mXTpS2pGTdQY9G02OvVqcyPldWhHjNuvDtXs5yu1FO9xmvZQdJFRv2hQ2bFy21ro/hyFn5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXES5EIIYXFHHeRKqTZKqUVKqW1Kqa1KqVtasmFCCCGOjOMYnhsA/qK1XqeUigSylVLztdbbWqhtQgghjsBRn5FrrQu11uuaL9cAOUDrlmqYEEKII9Mi75ErpTKAPsDqf3HfRKXUWqXU2pKKypbYnRBCiF9QWutj24BSEcAS4BGt9ef/7rHdevXRG997AhXmYabfVJYeG36AmiVfsWrozYxoFQJAOz089EMB9+rFALg69qEpuQv2alNJXAX9+NbMY8+QawBInfkwlbvyaP3Ym7jyTVXrXU8/g+fx9/hsq6lWf2tqGdruonHlVwA01dTjjAxn+9BJAHRZMRXb2bfiqMyHoB+AR3Mc3F7+OXuH3QiA/b7LybjwLHRjHQBXlA/mtbHd8dYWAmArzyPQpjfTd1Rz1g/PARBz8c0EI1vh2PEDAKH6ahxJ6dStMFXCQ+ffRfi2+RTNng1A6yuvJVhexP53PyDqkbfNNrbMZVHCcLp9OMW048/PE7PsHb5MPxeAc9Id2CvyOX+haffzu6biuOsVEjx23thgxmxiZw+VtkiK6gIALOo8kB7rluGZOBaAvnPmMO9AA1lPTQQg6+EnCOasYt8nX5J+5nAA9nw8jy5T7kA5nAAEEjuwrMzOcWmRABTVNpG2ZyHYzPmBLTKWvfG9ScuewfonpwOQ+cU3+J/7MxG3Pw/As8v2M2VQHD6X2UZjQBMdrGbadlOt/fq0WvzrF+ErKqL03MkAOG2KxsmXk/H0GwCsKgnRN8WLc9mHZhtDxlH3zJ8pn/gkADd9tIHbTu3E6b4N+HeuB6Bg5CQ2H6pljCcPgGBkK7TDhb14j+lbWk9sOUsIlpnxe8kzknE9kvGHNPsqGwE4PsXNgTq4/I01ALw3YSBKQfrObwA42Ok02vjy2e0wL1SzGnIJRiRQYo/FZTfV6r1OGyGt+bB1HwCu/uEV9iYNJN1p+h9c+hG1wydQ5QsC4AtoOvty+ZkOEUjsQPlL91B6xaMAdC1by737U7jthAwAotZ8zMG580m//HIAarOGod//G42VpkJ78fpcejx8L0WffMDbx/0ZgNHP30jmOYPwnj4egJUNcQyK8RFY+IHZbcBP+da92P7yIgAN912J+29vURcI0XbdRwAotxdHQjK+Dsebeas6SI03Bfe3LwMQ1qUfU2vaM2buwwC4osIJT44ndP5d5JSa/g9o2Mrel18ifZxZp/dXdufu4e3w7jbHkwrzUJbSl/1VZu13X/cO32ZeRFqUm17RZsyCi97Hf/J1VD18AwBvDr+Lqwekkd11kOnrtGuxn3AhT7U+AYC+65Yx4LsnKRj7VzrvXwDAbQVZTDmpw8/z9snWYka2j8P7yl8AiB/Qm7Cug8iL7gxA8vpPGbIkhScu70fY5ecA0GfhAkrqAySEm3e0vQUb0WFettrTAejm20OuN4sONTkAvFqcyBlZCQC0T4zK1lr35zCO6YxcKeUEZgLT/1OICyGE+HUcy6dWFPAmkKO1frblmiSEEOK/cSxn5McDlwEjlVIbmn9Ob6F2CSGEOEJH/fFDrfUyQLVgW4QQQhwF+WanEEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnAS5EEJYnNJa/2Y769+tk17x9wdZmTrq59t6J3v5ceAw7C4b/ZcvBmBDUT2DI+vQrnAAbDWHCOVtR/c8BYCgzYl9+QyqNmwAIGbCFJSvjlDOCpTdbjrmDmdD0gn01QfMcwp2s2j8I4xc/gUAgdVf4ux3CtoRBsC8Xmdw8vYlqFCAvX43AO1cjXy218/Ytub3XY0zhuiqvVRFtwOg1h8iOSxIdqmpTD/QUUQwJo1a7fy52vbWkgYSwp1k+PPNc+a+j/fsq2H/ZjMAbXsQWDMXZ7+TASif8XfGucbyVfpatM9Ua3cNPYfPSqPIiPEAkDn7Eb4Z+mcu7BQNQFXARoTLjsNXbbYZ8PP5QcW5ZQupG2Cqj8cUb6Fk5vskXHg1APWtOhFespP93vYA+O++jD3zchn9w3tmE7mb2JJ1NvEv34Lz7lcBiPPYqW8KUVJv+psS4WRN/6GcuNJUjc8PeEj3FxDcvtq0IxRkU8dz6LH9c+qOGwfA8rxqTqtbQ2M309+qxiDRbju+N+417br0QRKXvk7NiIkAVPiCtFnyd+pOnUTVPVcAkDpiIM5hFxJYaeZy78AryLJXwj6zHpbEHMeIpq34d64H4LP0C7ig+Gts3ihs4ZEA3LCnNZNHdsDjNHO754zR5L44g4sTKwDQdhe1UW2wffo4AE8mj+OvAyJpmP13IkeYqujaGUYgLoN9deYYytg0k7xeY3HazNyvzK/inKwYHFUFAOy2JdHOEyC0+ANsETEAOLL60BCfiXPVp2Ye8nNxpbalachFALir8tmpkshY8goAeQt/JOvOO9l4+18B6PinYbgye6JSOhCIywBgQ4mPGLeDjrXbAShN6Mbus09j4PP3ANC4bQ2O2ETUCWZOKkJOPttWzJi5DxNsMnPbeNtLZETacZbsBiDojUfpEKVvPAFA0vkXm8etXwpAQ0k5MaPOBnckX9S0AmBMfA0bg0l02zwDgH2fz2PTza9S3dgEwGVdY9A2Bz5t5mBVfg0jkkA1NbInZMZn3u5SrusRg9r4HQAzwo/nT13iKH3iVtP2656mC8Uof71Zcw3VBFt1wNZYwz3Zpi+P9nNQFdmG2OItAMxuaEOHuHB8gZCZt5gw4ku2sOsJM9cZF53D+xEnclVcIVXzzRqb3vta+t49nnUbDgEwoXAdTZ89Rd2YOwBIPPgjHw67nlG71wIQePpmUi+6mPL044iyB80aq9F0rtsOQdP/YExr9I5VBIpMRtlPvx50iPKAKdwWHfaPYzosNilba92fw5AzciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsDgJciGEsLhjCnKl1Gil1A6l1G6l1N0t1SghhBBH7qiDXCllB14BTgO6AuOUUl1bqmFCCCGOzLGckQ8Edmutc7XWfuAjYEzLNEsIIcSROuriy0qpscBorfWE5uuXAYO01pP+6XETgYnNV7sDW46+uf8TEoDS37sR/wfIOMgYgIzBT/7TOLTVWice7k5Hy7fn/0trPQ2YBqCUWvvvKkH/EcgYGDIOMgYgY/CTYx2HY3lr5SDQ5hfX05pvE0II8Rs6liD/EchSSrVTSrmAi4A5LdMsIYQQR+qo31rRWgeUUpOAeYAdeEtrvfU/PG3a0e7vf4iMgSHjIGMAMgY/OaZxOOo/dgohhPi/Qb7ZKYQQFidBLoQQFvebBPkf+av8Sql9SqnNSqkNSqm1zbfFKaXmK6V2Nf8b+3u3syUppd5SShUrpbb84rZ/2WdlvNi8NjYppfr+fi1vWYcZhweUUgeb18MGpdTpv7hvcvM47FBKnfr7tLplKaXaKKUWKaW2KaW2KqVuab79D7Me/s0YtNxa0Fr/qj+YP4TuAdoDLmAj0PXX3u//lR9gH5DwT7c9CdzdfPlu4Infu50t3OdhQF9gy3/qM3A68A2ggMHA6t+7/b/yODwA3P4vHtu1+dgIA9o1HzP237sPLTAGKUDf5suRwM7mvv5h1sO/GYMWWwu/xRm5fJX//28M8G7z5XeBc37HtrQ4rfVSoPyfbj5cn8cA72ljFRCjlEr5bVr66zrMOBzOGOAjrbVPa70X2I05dixNa12otV7XfLkGyAFa8wdaD/9mDA7nv14Lv0WQtwbyfnE9n3/fif81GvhOKZXd/N8VACRprQubLxcBSb9P035Th+vzH3F9TGp+2+CtX7yt9j8/DkqpDKAPsJo/6Hr4pzGAFloL8sfOX99QrXVfzP8SeaNSatgv79TmtdQf6jOgf8Q+/8JUoAPQGygEnvl9m/PbUEpFADOBW7XW1b+874+yHv7FGLTYWvgtgvwP/VV+rfXB5n+LgVmYl0iHfnq52Pxv8e/Xwt/M4fr8h1ofWutDWuug1joEvM4/XjL/z46DUsqJCbDpWuvPm2/+Q62HfzUGLbkWfosg/8N+lV8p5VVKRf50GTgF878/zgHGNz9sPDD792nhb+pwfZ4DXN78aYXBQNUvXnL/z/mn93vP5R//G+gc4CKlVJhSqh2QBaz5rdvX0pRSCngTyNFaP/uLu/4w6+FwY9Cia+E3+qvt6Zi/1O4Bpvzef0X+rX4wn9TZ2Pyz9ae+A/HAQmAXsACI+73b2sL9noF5qdiEeX/v6sP1GfPphFea18ZmoP/v3f5feRzeb+7npuYDNuUXj5/SPA47gNN+7/a30BgMxbxtsgnY0Pxz+h9pPfybMWixtSBf0RdCCIuTP3YKIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTFSZALIYTF/T9VMkc7rKJLHAAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["\"\"\"\n","(2) Make the Source Masking for the sample_x\n","Source Masking is a tensor of shape (MAX_LENGTH), in which Padded tokens are True, and the rest of the tokens are False.\n","\"\"\"\n","x, y = next(iter(train_dataloader))\n","sample_x = x[0]\n","\n","########## Your Code #########\n","source_mask = (sample_x == dataset.input_lang_pad)\n","\n","print(\"source mask = pad masking\")\n","print(source_mask.squeeze().tolist())\n","##############################"],"metadata":{"id":"GBizStE9POJR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670805975458,"user_tz":-540,"elapsed":4,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"f634c4c4-bbba-44ca-c628-4606ae97370e"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["source mask = pad masking\n","[False, False, False, False, False, False, False, False, False, True]\n"]}]},{"cell_type":"code","source":["\"\"\"\n","(2) Make the Target Maskings for the sample_y.\n","\n","(2)-1 : pad_mask_neg\n","pad_mask_neg is a tensor of shape (MAX_LENGTH), in which Padded tokens are False, and the rest of the tokens are True.\n","\n","(2)-2 : sub_mask \n","sub_mask is a tensor of shape (MAX_LENGTH, MAX_LENGTH), in which only the tokens of the current timestep are True.\n","You can use torch.tril function.\n","\n","(2)-3 : target mask (MAX_LENGTH, MAX_LENGTH)\n","target mask is a tensor of shape  (MAX_LENGTH, MAX_LENGTH), \n","in which both of the not-padded tokens and the tokens of the current timestep are True.\n","\n","\"\"\"\n","x, y = next(iter(train_dataloader))\n","sample_y = y[0]\n","\n","########## Your Code #########\n","pad_mask_neg = (sample_y != dataset.output_lang_pad).unsqueeze(1)\n","sub_mask = torch.tril(torch.ones((MAX_LENGTH, MAX_LENGTH))).bool()\n","target_mask = pad_mask_neg.permute(2,1,0) & sub_mask.unsqueeze(0)\n","\n","print(\"\\ntarget mask = pad masking + subsequent masking\")\n","print(pad_mask_neg.squeeze().tolist())\n","print()\n","print(sub_mask.squeeze())\n","print()\n","print(target_mask.squeeze())\n","##############################"],"metadata":{"id":"PfQby0Nz59_9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670806110301,"user_tz":-540,"elapsed":12,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"fadea5aa-a4f9-469f-dbfb-0fbe48bdeef4"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","target mask = pad masking + subsequent masking\n","[True, True, True, True, True, True, True, False, False, False]\n","\n","tensor([[ True, False, False, False, False, False, False, False, False, False],\n","        [ True,  True, False, False, False, False, False, False, False, False],\n","        [ True,  True,  True, False, False, False, False, False, False, False],\n","        [ True,  True,  True,  True, False, False, False, False, False, False],\n","        [ True,  True,  True,  True,  True, False, False, False, False, False],\n","        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n","        [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n","        [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n","        [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n","        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n","\n","tensor([[ True, False, False, False, False, False, False, False, False, False],\n","        [ True,  True, False, False, False, False, False, False, False, False],\n","        [ True,  True,  True, False, False, False, False, False, False, False],\n","        [ True,  True,  True,  True, False, False, False, False, False, False],\n","        [ True,  True,  True,  True,  True, False, False, False, False, False],\n","        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n","        [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n","        [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n","        [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n","        [ True,  True,  True,  True,  True,  True,  True, False, False, False]])\n"]}]},{"cell_type":"markdown","source":["OUTPUT EXAMPLE\n","```\n","target mask = pad masking + subsequent masking\n","[True, True, True, True, True, True, False, False, False, False]\n","\n","tensor([[ True, False, False, False, False, False, False, False, False, False],\n","        [ True,  True, False, False, False, False, False, False, False, False],\n","        [ True,  True,  True, False, False, False, False, False, False, False],\n","        [ True,  True,  True,  True, False, False, False, False, False, False],\n","        [ True,  True,  True,  True,  True, False, False, False, False, False],\n","        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n","        [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n","        [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n","        [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n","        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n","\n","tensor([[ True, False, False, False, False, False, False, False, False, False],\n","        [ True,  True, False, False, False, False, False, False, False, False],\n","        [ True,  True,  True, False, False, False, False, False, False, False],\n","        [ True,  True,  True,  True, False, False, False, False, False, False],\n","        [ True,  True,  True,  True,  True, False, False, False, False, False],\n","        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n","        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n","        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n","        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n","        [ True,  True,  True,  True,  True,  True, False, False, False, False]])\n","```\n","\n"],"metadata":{"id":"BuwL-6SI9jyh"}},{"cell_type":"code","source":["\"\"\"\n","(3) Implement Seq2seq model with Transformer\n","\"\"\"\n","\n","from torch.nn import Transformer ## 과제에서는 사용하면 안됨\n","\n","class TransSeq2Seq(nn.Module):\n","    def __init__(self, hid_dim, ff_dim, n_heads, n_enc_layers, n_dec_layers, dropout_p):\n","        super().__init__()\n","        \"\"\"\n","        - Define self.input_emb and self.output_emb layer same as in Q(2) and Q(3).\n","        - self.dropout will be used after adding the embedded_pos to the embedded_x and embedded_y.\n","        - use 'gelu' for activation function in Transformer layer\n","        \"\"\"\n","        self.input_emb = nn.Embedding(in_dim, hid_dim)\n","        self.output_emb = nn.Embedding(out_dim, hid_dim)\n","        self.pos_emb = nn.Embedding(MAX_LENGTH, hid_dim)\n","        ########## Your Code #########\n","        self.transformer = Transformer(d_model=hid_dim,\n","                                       nhead = n_heads,\n","                                       num_encoder_layers = n_enc_layers,\n","                                       num_decoder_layers = n_dec_layers,\n","                                       dim_feedforward=ff_dim,\n","                                       dropout = dropout_p,\n","                                       activation='gelu')\n","        self.dropout = nn.Dropout(dropout_p)\n","        ##############################\n","\n","    def forward(self, src, trg):\n","        \"\"\"\n","        1) Get the embedded_x using self.input_emb and src (source)\n","        2) Get the embedded_y using self.input_emb and trg (target)\n","        3) Get the embedded_pos using self.pos_emb\n","        4) Apply the dropout to embedded_x and embedded_y\n","        5) Get the output using self.transformer\n","        \"\"\"\n","        ########## Your Code #########\n","        embedded_pos = self.pos_emb(torch.arange(MAX_LENGTH).unsqueeze(1))\n","        embedded_x = self.input_emb(src)\n","        embedded_y = self.output_emb(trg)\n","\n","        embedded_x = self.dropout(torch.sum(embedded_x + embedded_pos, dim=1))\n","        embedded_y = self.dropout(torch.sum(embedded_y + embedded_pos, dim=1))\n","\n","        return self.transformer(embedded_x, embedded_y)\n","        ##############################\n","\n"],"metadata":{"id":"h-NgAbGqIFco","executionInfo":{"status":"ok","timestamp":1670806494595,"user_tz":-540,"elapsed":5,"user":{"displayName":"신성구","userId":"07958575584326347145"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","(4) Define the Transformer model.\n","Compute and print the output of the model using sample_x and sample_y as inputs.\n","\"\"\"\n","########## Your Code #########\n","emb_dim = 256\n","hid_dim = emb_dim\n","ff_dim = 1024\n","n_heads = 8\n","n_enc_layers = 3\n","n_dec_layers = 5\n","dropout_p = 0.1\n","\n","model = TransSeq2Seq(hid_dim, ff_dim, n_heads, n_enc_layers, n_dec_layers, dropout_p)\n","output = model(sample_x, sample_y)\n","print(output.shape) # (10, 256)\n","##############################"],"metadata":{"id":"EljQjKn0JYkc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670806496325,"user_tz":-540,"elapsed":3,"user":{"displayName":"신성구","userId":"07958575584326347145"}},"outputId":"a4eb88cd-cb8c-42c6-ebaa-e87dc4f797c3"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 256])\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZDma7XS2eKSQ"},"source":["# Practice 3. BERT fine-tuning"]},{"cell_type":"code","source":["\"\"\"\n","In this Practice, you are not going to solve questions.\n","Run the codes and see how we can use pretrained BERT.\n","\"\"\""],"metadata":{"id":"bWP5GmVkDOz1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WaKMY_DfUDmC"},"source":["**References**\n","\n","1. [huggingface BERT documentation](https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#bert)\n","\n","2. [masked language modelling with bert](https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c)\n","\n","3. [fine-tuning bert for text classification in pytorch](https://luv-bansal.medium.com/fine-tuning-bert-for-text-classification-in-pytorch-503d97342db2)\n","\n","4. [pytorch sentiment classification github](https://github.com/clairett/pytorch-sentiment-classification/tree/master/data/SST2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gUQDOmLDeMPo"},"outputs":[],"source":["from transformers import BertTokenizer, BertForMaskedLM\n","from tqdm import tqdm\n","from torch.optim import AdamW\n","\n","MX_LENGTH = 50\n","BATCH_SIZE = 32\n","MASK_RATIO = 0.2 # 0.15 논문\n","EPOCHS = 3\n","learning_rate = 5e-3 #5e-5 -> loss: 1~2 after one epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Fp3gP2nfTga"},"outputs":[],"source":["# Prepare Dataset\n","class BertDataset(Dataset):\n","    def __init__(self, tokenizer, max_length=512, mask_ratio=0.15):\n","        super(BertDataset, self).__init__()\n","        self.train_csv=pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n","        self.tokenizer=tokenizer\n","        self.target=self.train_csv.iloc[:,1]\n","        self.max_length = max_length\n","        self.mask_ratio = mask_ratio\n","\n","    def __len__(self):\n","        return len(self.train_csv)\n","    \n","    def __getitem__(self, index):\n","        inputs = self.tokenizer.encode_plus(text=self.train_csv.iloc[index,0],\n","                                            padding='max_length',\n","                                            truncation=True,\n","                                            add_special_tokens=True,\n","                                            return_tensors='pt',\n","                                            max_length=self.max_length,)\n","\n","        return {'input_ids': inputs[\"input_ids\"].clone().detach(),\n","                'token_type_ids': inputs[\"token_type_ids\"].clone().detach(),}\n","\n","    def _apply_masking(self, x):\n","        rand = torch.rand(x['input_ids'].shape)\n","        mask = (rand < self.mask_ratio) * (x['input_ids'] != 101) * (x['input_ids'] != 102) * (x['input_ids'] != 0) # t/f tensor\n","\n","        selection = torch.flatten(mask[0].nonzero()).tolist() # idxs masked\n","        x['input_ids'][0, selection] = 103 # apply MASK token\n","\n","        return x\n","\n","    def collate_fn(self, data):\n","        batch={'input_ids':None, 'labels':None}\n","        \n","        # copy ids\n","        tmp = [item['input_ids'] for item in data]\n","        batch['labels'] = torch.stack(tmp).squeeze(1)\n","        \n","        # create mask tensor\n","        data = list(map(self._apply_masking, data))\n","        tmp = [item['input_ids'] for item in data]\n","        batch['input_ids'] = torch.stack(tmp).squeeze(1)\n","\n","        return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXoIN3AgAlnb"},"outputs":[],"source":["# Dataset\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","dataset= BertDataset(tokenizer, max_length=MX_LENGTH, mask_ratio=MASK_RATIO)\n","\n","# Dataloader\n","dataloader=DataLoader(dataset=dataset,batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"128XnwZsiZoD"},"outputs":[],"source":["# Explore Dataset \n","for i, batch_data in enumerate(dataloader):\n","    if i==0:\n","        print(\"data shape\")\n","        print(batch_data['labels'].shape) # batch_size * max_length\n","        print()\n","        print(\"before masking\")\n","        print(batch_data['labels'][0]) # 101: CLS, 102: SEP / we use single sentence for fine-tuning task\n","        print()\n","        print(\"after masking (MASK = 103)\")\n","        print(batch_data['input_ids'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9xtPgQlh_y4"},"outputs":[],"source":["# Pretrained model\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OC9pa0_QxYvA"},"outputs":[],"source":["# Train\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n","model.train()\n","optim = AdamW(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(EPOCHS):\n","    loop = tqdm(dataloader, leave=True)\n","    for batch in loop:\n","        optim.zero_grad()\n","        \n","        input_ids = batch['input_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","        \n","        outputs = model(input_ids, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optim.step()\n","\n","        loop.set_description(f'Epoch {epoch}')\n","        loop.set_postfix(loss=loss.item())"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}