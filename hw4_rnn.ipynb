{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ],
      "metadata": {
        "id": "xvh969t1VJzj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO4EhYqCTdbt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Change directory to where this file is located\n",
        "\"\"\"\n",
        "%cd 'COPY&PASTE FILE DIRECTORY HERE'"
      ],
      "metadata": {
        "id": "gyi5Ljv8TpjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchdata"
      ],
      "metadata": {
        "id": "fDk49qpsU_BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchtext\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data.functional import to_map_style_dataset"
      ],
      "metadata": {
        "id": "NMAuwnOwVBOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import modules you need\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "I6fOONHU7aMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "print(\"Using PyTorch version: {}, Device: {}\".format(torch.__version__, DEVICE)) ## should be 1.11.0 and cuda\n",
        "print(\"Using torchtext version: {}\".format(torchtext.__version__)) ## should be 0.12.0"
      ],
      "metadata": {
        "id": "yn5FYiiQVC2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "E4E1p3w-VLTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load AG_NEWS dataset and set up the tokenizer and encoder pipeline.\n",
        "\n",
        "Do NOT modify.\n",
        "\"\"\"\n",
        "\n",
        "train_data, test_data = torchtext.datasets.AG_NEWS(root='./data')\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "encoder = build_vocab_from_iterator(tokens(train_data), specials=[\"<unk>\"])\n",
        "encoder.set_default_index(encoder[\"<unk>\"])\n",
        "\n",
        "text_pipeline = lambda x: encoder(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "metadata": {
        "id": "NXSORPxqn7lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Creates a batch of encoded text, label and token length tensors.\n",
        "\n",
        "    Question (a)\n",
        "    - Length of token sequence in each batch is determined by \n",
        "      the average of token length of all sequences in each batch.\n",
        "    - Text tensors are stacked with dimension of (TOKEN_LENGTH, BATCH),\n",
        "      for easier process in RNN model.\n",
        "    - Token length tensors are used to index the last valid hidden token for classification.\n",
        "\n",
        "    Inputs\n",
        "    - list of tuples, each containing an integer label and a text input\n",
        "    - number of tuples in the list == BATCH SIZE\n",
        "    Returns\n",
        "    - text_list: batch of encoded long type text tensors with size (TOKEN_LENGTH, BATCH)\n",
        "    - label_list: batch of label tensors with size (BATCH)\n",
        "    - len_list: batch of token length tensors with size (BATCH)\n",
        "    \"\"\"\n",
        "\n",
        "    text_list, label_list, len_list = [], [], []\n",
        "    \n",
        "    ### COMPLETE HERE ###\n",
        "    \n",
        "    ### COMPLETE HERE ###\n",
        "    \n",
        "    assert text_list.size(1) == len(batch)\n",
        "\n",
        "    return text_list, label_list, len_list"
      ],
      "metadata": {
        "id": "jpb0L6hvmh2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load the data loader.\n",
        "\n",
        "Do NOT modify.\n",
        "\"\"\"\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "train_dataset = to_map_style_dataset(train_data)\n",
        "test_dataset = to_map_style_dataset(test_data)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "nJXoUuq0NgiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Print out the first batch in the train loader.\n",
        "Check if the collate function is implemented correctly.\n",
        "\n",
        "Do NOT modify.\n",
        "\"\"\"\n",
        "\n",
        "for batch_x, batch_y, len_x in train_dataloader:\n",
        "    print(batch_x[:10])\n",
        "    print(batch_y[:10])\n",
        "    print(len_x[:10])\n",
        "    break"
      ],
      "metadata": {
        "id": "4X2_aQKhxWV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Plot the sequence length distribution of the batches in the train dataloader.\n",
        "Make sure that all batches have difference sequence lengths.\n",
        "\n",
        "Do NOT modify.\n",
        "\"\"\"\n",
        "\n",
        "batch_len = []\n",
        "for batch_x, _, _ in train_dataloader:\n",
        "    batch_len.append(batch_x.size(0))\n",
        "plt.hist(batch_len)"
      ],
      "metadata": {
        "id": "95s-BUHc37O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "kR_cAYRUVbor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, num_class):\n",
        "        \"\"\"\n",
        "        Define the model weight parameters and initialize the weights.\n",
        "\n",
        "        Question (b)\n",
        "        - Complete the dimension and shape of the weights and biases.\n",
        "        - Use the model parameters (vocab_size, input_size, hidden_size, num_class).\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        ### COMPLETE HERE ###\n",
        "        whh_size = \n",
        "        wxh_size = \n",
        "        why_size = \n",
        "        bhh_size = \n",
        "        bxh_size = \n",
        "        bhy_size = \n",
        "        ### COMPLETE HERE ###\n",
        "\n",
        "        kwargs = {'device': DEVICE, 'dtype': torch.float}\n",
        "        self.hidden = hidden_size\n",
        "        self.num_class = num_class\n",
        "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
        "        self.W_hh = nn.parameter.Parameter(torch.empty(whh_size, **kwargs))\n",
        "        self.W_xh = nn.parameter.Parameter(torch.empty(wxh_size, **kwargs))\n",
        "        self.W_hy = nn.parameter.Parameter(torch.empty(why_size, **kwargs))\n",
        "        self.b_hh = nn.parameter.Parameter(torch.empty(bhh_size, **kwargs))\n",
        "        self.b_xh = nn.parameter.Parameter(torch.empty(bxh_size, **kwargs))\n",
        "        self.b_hy = nn.parameter.Parameter(torch.empty(bhy_size, **kwargs))\n",
        "\n",
        "        self.init_parameters()\n",
        "\n",
        "    def init_parameters(self):\n",
        "        \"\"\"\n",
        "        Initialize the parameters with Kaiming uniform initialization.\n",
        "\n",
        "        Do NOT modify this method.\n",
        "        \"\"\"\n",
        "        nn.init.kaiming_uniform_(self.W_hh, a=math.sqrt(5))\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_hh)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.b_hh, -bound, bound)\n",
        "        nn.init.kaiming_uniform_(self.W_xh, a=math.sqrt(5))\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_xh)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.b_xh, -bound, bound)\n",
        "        nn.init.kaiming_uniform_(self.W_hy, a=math.sqrt(5))\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_hy)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.b_hy, -bound, bound)\n",
        "\n",
        "    def forward(self, inputs, length):\n",
        "        \"\"\"\n",
        "        Question (c)\n",
        "        - Passes a sequence of tokens into the recurrent network.\n",
        "        - Randomly initialize h_0 with appropriate shape.\n",
        "        - We do not want to use a hidden cell of a zero-padded token for classification!\n",
        "        - Index the hidden state of the last valid token (excluding the zero-padding)\n",
        "          based on the token length of each example in the batch.\n",
        "\n",
        "        Inputs\n",
        "        - a batch of encoded token sequences with shape (SEQ_LEN, BATCH_SIZE)\n",
        "        - a batch of token lengths with shape (BATCH_SIZE)\n",
        "        Returns\n",
        "        - Softmax probabilites for each class with shape (BATCH_SIZE, NUM_CLASS)\n",
        "        \"\"\"\n",
        "        \n",
        "        ### COMPLETE HERE ###\n",
        "\n",
        "        ### COMPLETE HERE ###\n",
        "\n",
        "        return softmax_probs\n",
        "    \n",
        "    def compute_loss(self, prediction, label):\n",
        "        \"\"\"\n",
        "        Question (d)\n",
        "        - Compute the cross entropy loss and the number of correct predictions\n",
        "        - Do NOT use loss function in torch.nn library ex) nn.CrossEntropyLoss()\n",
        "        - Hint: use torch.nn.functional.one_hot(tensor, num_classes=?) to generate one-hot encodings\n",
        "\n",
        "\n",
        "        Inputs\n",
        "        - prediction: output from self.forward(inputs) with shape (BATCH_SIZE, NUM_CLASS)\n",
        "        - label: integer labels of the batch inputs with shape (BATCH_SIZE)\n",
        "        Returns\n",
        "        - cross entropy loss of the batch (float) and number of correct predictions (integer)\n",
        "        \"\"\"\n",
        "        loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        ### COMPLETE HERE ###\n",
        "        \n",
        "        ### COMPLETE HERE ###\n",
        "\n",
        "        return loss, correct"
      ],
      "metadata": {
        "id": "9CMsOucsVbVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Modules"
      ],
      "metadata": {
        "id": "zIdypCAFscfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScheduledOptim():\n",
        "    \"\"\"\n",
        "    Learning rate scheduler.\n",
        "\n",
        "    Do NOT modify.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, n_warmup_steps, decay_rate):\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.decay = decay_rate\n",
        "        self.n_steps = 0\n",
        "        self.initial_lr = optimizer.param_groups[0]['lr']\n",
        "        self.current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self._optimizer.zero_grad()\n",
        "    \n",
        "    def step(self):\n",
        "        self._optimizer.step()\n",
        "    \n",
        "    def get_lr(self):\n",
        "        return self.current_lr\n",
        "    \n",
        "    def update(self):\n",
        "        if self.n_steps < self.n_warmup_steps:\n",
        "            lr = self.n_steps / self.n_warmup_steps * self.initial_lr\n",
        "        elif self.n_steps == self.n_warmup_steps:\n",
        "            lr = self.initial_lr\n",
        "        else:\n",
        "            lr = self.current_lr * self.decay\n",
        "        \n",
        "        self.current_lr = lr\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        self.n_steps += 1"
      ],
      "metadata": {
        "id": "o9lUSQJjsfrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Functions for training and evaluating the model.\n",
        "\n",
        "Question (e)\n",
        "- There has been minor changes with the model forward operation and loss computation.\n",
        "  Compare the updates with the train, evaluate functions that we have previously used,\n",
        "  and complete the train and evaluate function that works for the current model architecture.\n",
        "- Use the methods of the scheduler to perform necessary operations on the optimizer.\n",
        "- Do NOT change the arguments given to the train, evaluate functions.\n",
        "\"\"\"\n",
        "\n",
        "def train(model, train_loader, scheduler):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    ### COMPLETE HERE ###\n",
        "    tqdm_bar = tqdm(train_loader)\n",
        "\n",
        "    for text, label, length in tqdm_bar:\n",
        "        text = text.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "        length = length.to(DEVICE)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = 100. * correct / len(train_loader.dataset)\n",
        "    ### COMPLETE HERE ###\n",
        "    \n",
        "    return train_loss, train_acc\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    ### COMPLETE HERE ###\n",
        "    \n",
        "    ### COMPLETE HERE ###\n",
        "\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "pgisHLbUsLkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "a1qXY-CDNIia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question (f)\n",
        "- Train your RNN model and obtain the test accuracy of 70%.\n",
        "- Select the input size, hidden size of your choice\n",
        "- Try various optimizer type, learning rate and scheduler options for the best performance.\n",
        "\"\"\"\n",
        "\n",
        "### COMPLETE HERE ###\n",
        "EPOCHS = 0\n",
        "vocab_size = 0\n",
        "input_size = 0\n",
        "hidden_size = 0\n",
        "num_class = 0\n",
        "\n",
        "model = None\n",
        "optimizer = None\n",
        "scheduler = None\n",
        "### COMPLETE HERE ###\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    loss_train, accu_train = train(model, train_dataloader, scheduler)\n",
        "    loss_val, accu_val = evaluate(model, valid_dataloader)\n",
        "    lr = scheduler.get_lr()\n",
        "    print('-' * 83)\n",
        "    print('| end of epoch {:2d} | lr: {:5.4f} | train accuracy: {:8.3f} | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch, lr, accu_train, accu_val))\n",
        "    print('-' * 83)"
      ],
      "metadata": {
        "id": "EA061dLNNKg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JDMiURv9hjfb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}