{"cells":[{"cell_type":"markdown","id":"b945e8df","metadata":{"id":"b945e8df"},"source":["# Convolution Function"]},{"cell_type":"code","source":["\"\"\"\n","Question (a)\n","\n","Implement your own conv functions which performs convolution operation without using any neural network packages.\n","Make sure to handle all possible edge cases to receive full credits.\n","Keep in mind that height and width of the given image or filter are not always the same.\n","\"\"\"\n","\n","import numpy as np\n","\n","\n","def convolution_naive(image, filter, stride=1, padding=0):\n","    \"\"\"Performs 2D convolution operation with \"4 nested for-loops\".\n","\n","    Args:\n","    - image: 2D numpy array\n","    - filter: 2D numpy array\n","    - stride, padding: integers\n","\n","    Returns:\n","    - 2D numpy array : convolution results of the given image and filter.\n","    - Return None if stride is not compatible. (ex. image of 5*5 with filter 2*2 with stride 2, padding 0)\n","    - Return None if filter is larger than the given image.\n","    \"\"\"\n","\n","    ##### YOUR CODE #####\n","\n","    #####################\n","    \n","    return output\n","\n","\n","def convolution_vectorized(image, filter, stride=1, padding=0):\n","    \"\"\"Performs 2D convolution operation with \"less than or equal to 2 nested for-loops\".\n","\n","    Args:\n","    - image: 2D numpy array \n","    - filter: 2D numpy array\n","    - stride, padding: integers\n","\n","    Returns:\n","    - 2D numpy array : convolution results of the given image and filter..\n","    - Return None if stride is not compatible. (ex. image of 5*5 with filter 2*2 with stride 2, padding 0)\n","    - Return None if filter is larger than the given image.\n","    \"\"\"\n","\n","    ##### YOUR CODE #####\n","\n","    #####################\n","    \n","    return output"],"metadata":{"id":"adJPGBJbLkN7"},"id":"adJPGBJbLkN7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = np.random.randint(10, size=(256, 256))\n","filter = np.random.randint(10, size=(16, 16))\n","stride = 1\n","padding = 0\n","\n","print(\"Compare the time complexity of 2 convolution operations\")\n","print(\"1. Convolution operation with 4 nested loops\")\n","%timeit -n 3 -r 1 convolution_naive(image, filter, stride, padding)\n","print(\"2. Convolution operation with less than or equal to 2 nested loops\")\n","%timeit -n 3 -r 1 convolution_vectorized(image, filter, stride, padding)"],"metadata":{"id":"LehnZPO1FJo1"},"id":"LehnZPO1FJo1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = np.array([[1, 2, 3, 2, 1], [2, 3, 4, 5, 6], [-1, -2, -3, -4, -5], [0, 0, 1, 0, 0], [7, 1, 7, 1, 7]])\n","filter = np.array([[1, 0], [0, 1]])\n","stride = 3\n","padding = 0\n","\n","# Expected Result for each convolution functions.\n","# [[4. 8.]\n","#  [1. 7.]]\n","print(convolution_naive(image, filter, stride, padding))\n","print(convolution_vectorized(image, filter, stride, padding))"],"metadata":{"id":"3JV8AbLaHo2r"},"id":"3JV8AbLaHo2r","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"mDOdbok4zVE4","metadata":{"id":"mDOdbok4zVE4"},"source":["# Colab Setup"]},{"cell_type":"code","execution_count":null,"id":"-NvjxSCiMfMq","metadata":{"id":"-NvjxSCiMfMq"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"oS5qDEF5MftN","metadata":{"id":"oS5qDEF5MftN"},"outputs":[],"source":["\"\"\"\n","Change directory to where this file is located\n","\"\"\"\n","# %cd 'COPY&PASTE FILE DIRECTORY HERE'"]},{"cell_type":"markdown","id":"giBbJLW-zhS-","metadata":{"id":"giBbJLW-zhS-"},"source":["# Import Modules"]},{"cell_type":"code","execution_count":null,"id":"KjGAWailzgP1","metadata":{"id":"KjGAWailzgP1"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, datasets"]},{"cell_type":"code","execution_count":null,"id":"55f78236","metadata":{"id":"55f78236"},"outputs":[],"source":["\"\"\"\n","import modules you need\n","\"\"\"\n"]},{"cell_type":"markdown","id":"2f33b210","metadata":{"id":"2f33b210"},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"id":"b7112995","metadata":{"id":"b7112995"},"outputs":[],"source":["def plot_dataset(dataloader, grid_width=8, grid_height=2, figure_width=12, figure_height=3, y_hats=None):\n","    \"\"\"\n","    Plots image and labels.\n","\n","    Do NOT modify this function.\n","    \"\"\"\n","    images, labels = next(iter(dataloader))\n","    f, ax = plt.subplots(grid_height, grid_width)\n","    f.set_size_inches(figure_width, figure_height)\n","    img_idx = 0\n","    for i in range(0, grid_height):\n","        for j in range(0, grid_width):\n","            image = images[img_idx]\n","            label = labels[img_idx]\n","            title_color = 'k'\n","            if y_hats is None:\n","                label_idx = int(label)\n","            else:\n","                label_idx = int(y_hats[img_idx])\n","                if int(labels[img_idx]) != label_idx:\n","                    title_color = 'r'\n","            label = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'][label_idx]\n","            ax[i][j].axis('off')\n","            ax[i][j].set_title(label, color=title_color)\n","            ax[i][j].imshow(np.transpose(image, (1, 2, 0)), aspect='auto')\n","            img_idx += 1\n","        plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0.25)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"-ANUeGuyNGua","metadata":{"id":"-ANUeGuyNGua"},"outputs":[],"source":["def train(model, train_loader, optimizer):\n","    \"\"\"\n","    Trains the model with training data.\n","\n","    Do NOT modify this function.\n","    \"\"\"\n","    model.train()\n","    tqdm_bar = tqdm(train_loader)\n","    for batch_idx, (image, label) in enumerate(tqdm_bar):\n","        image = image.to(DEVICE)\n","        label = label.to(DEVICE)\n","        optimizer.zero_grad()\n","        output = model(image)\n","        loss = criterion(output, label)\n","        loss.backward()\n","        optimizer.step()\n","        tqdm_bar.set_description(\"Epoch {} - train loss: {:.6f}\".format(epoch, loss.item()))\n","\n","\n","def evaluate(model, test_loader):\n","    \"\"\"\n","    Evaluates the trained model with test data.\n","\n","    Do NOT modify this function.\n","    \"\"\"\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for image, label in tqdm(test_loader):\n","            image = image.to(DEVICE)\n","            label = label.to(DEVICE)\n","            output = model(image)\n","            test_loss += criterion(output, label).item()\n","            prediction = output.max(1, keepdim=True)[1]\n","            correct += prediction.eq(label.view_as(prediction)).sum().item()\n","    \n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = 100. * correct / len(test_loader.dataset)\n","    return test_loss, test_accuracy"]},{"cell_type":"markdown","id":"4a242b15","metadata":{"id":"4a242b15"},"source":["# CIFAR-10 Data Augmentation"]},{"cell_type":"code","execution_count":null,"id":"aSH0yYaO2tBH","metadata":{"id":"aSH0yYaO2tBH"},"outputs":[],"source":["def gaussian_smoothing(image, filter_size=3, sigma=1.0):\n","    \"\"\"\n","    Inputs\n","    - image: an input image of shape (32,32,3).\n","    Returns\n","    - image: image blurred with a Gaussian Filter.\n","\n","    Do NOT modify this function.\n","    \"\"\"\n","    center = (filter_size-1)/2\n","    gaussian_filter = np.zeros((filter_size, filter_size))\n","    for row in range(filter_size):\n","        for col in range(filter_size):\n","            gaussian_filter[row, col] = np.exp((-(row-center) ** 2 - (col-center) ** 2) / (2 * sigma ** 2)) / (2 * np.pi * sigma ** 2)\n","    gaussian_filter = gaussian_filter / np.sum(gaussian_filter)\n","    image = cv2.filter2D(image, -1, gaussian_filter)\n","    return image\n","\n","\n","def color_jitter(image):\n","    \"\"\"\n","    Inputs\n","    - image: an input image of shape (32,32,3).\n","    Returns\n","    - image: image blurred with a Gaussian Filter.\n","\n","    Do NOT modify this function.\n","    \"\"\"\n","    image = cv2.convertScaleAbs(image, alpha=1.5, beta=20)\n","    return image"]},{"cell_type":"code","source":["def horizontal_flip(image):\n","    \"\"\"Flips the image horizontally.\n","\n","    Question (b)\n","    - Do not use TorchVision or OpenCV library for this question.\n","    - You can solve the problem using numpy only.\n","\n","    Inputs\n","    - image: an input image of shape (32, 32, 3).\n","\n","    Returns\n","    - image: a horizontally-flipped image of shape (32, 32, 3).\n","    \"\"\"\n","    ##### YOUR CODE #####\n","\n","    #####################\n","\n","    return image"],"metadata":{"id":"PQU4Yk_oPEVB"},"id":"PQU4Yk_oPEVB","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"VNXqg19fvtC5","metadata":{"id":"VNXqg19fvtC5"},"outputs":[],"source":["\"\"\"\n","Visualize how the augmentations are applied to a single image.\n","\n","Do NOT modify.\n","\"\"\"\n","\n","raw_test = datasets.CIFAR10(root=\"./CIFAR_10\", train=False, download=True)\n","\n","f, ax = plt.subplots(1, 5)\n","f.set_size_inches(12, 3)\n","x = raw_test[0][0]\n","x = np.array(x)\n","g = gaussian_smoothing(x)\n","j = color_jitter(x)\n","h = horizontal_flip(x)\n","title = ['original image', 'gaussian filter', 'color jitter', 'horizontal flip', 'original image']\n","for i, img in enumerate([x, g, j, h, x]):\n","    ax[i].imshow(img)\n","    ax[i].axis('off')\n","    ax[i].set_title(title[i], color='k')"]},{"cell_type":"code","execution_count":null,"id":"24wjKV1OCiP5","metadata":{"id":"24wjKV1OCiP5"},"outputs":[],"source":["### \n","# Question (b)\n","# Briefly explain whether horizontal flip is a good augmentation method for image classification task.\n","###"]},{"cell_type":"markdown","id":"weSZOvYtBtP0","metadata":{"id":"weSZOvYtBtP0"},"source":["Write your answer to question (b) in this cell.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"0SQnrsBsUmJX","metadata":{"id":"0SQnrsBsUmJX"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, train, prob=0.5, data_dir=\"./CIFAR_10\"):\n","        \"\"\"\n","        Do NOT modify this method.\n","        \"\"\"\n","        self.data = datasets.CIFAR10(root=data_dir, train=train, download=True)\n","        self.prob = prob\n","\n","    def __len__(self):\n","        \"\"\"\n","        Do NOT modify this method.\n","        \"\"\"\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Do NOT modify this method.\n","        \"\"\"\n","        return self.data[idx]\n","    \n","    def transform(self, image):\n","        \"\"\"\n","        Apply stochastic data augmentation to the given image.\n","\n","        Question (c)\n","        - Convert the given RGB image into BGR scale using opencv library.\n","        - Apply random augmentation (gaussian smoothing, color jitter, and horizontal flip).\n","        - Random augmentation is applied with the probability of self.prob.\n","        - If self.prob = 0.5, 5 out of 10 images will be augmented on average.\n","        - Convert the augmented image back to RGB scale for training.\n","\n","        Inputs\n","        - image: numpy array of an input image of shape (32,32,3).\n","        Returns\n","        - image: numpy array of the augmented input image with shape (32,32,3).\n","        \"\"\"\n","        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","\n","        ##### YOUR CODE #####\n","\n","        #####################\n","        \n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        return image\n","\n","    def collate_fn(self, data):\n","        \"\"\"\n","        Creates a batch of images and label tensors.\n","\n","        Question (d)\n","        - Convert each image in the batch from PIL image to numpy array.\n","        - Transform the image using self.transform method to apply random augmentation.\n","        - Normalize the transformed image by mapping the range [0, 255] to range [0, 1].\n","        - Transpose the (H * W * C) format of the image into (C * H * W) format.\n","          - To be specific, the dimension of the original image is (32, 32, 3).\n","          - We want the dimension of the transposed image to be (3, 32, 32).\n","        - Convert the batch of preprocessed images into PyTorch float tensors.\n","        - Convert the batch of labels into PyTorch long tensors.\n","        - Do NOT use torchvision.transforms library!\n","\n","        Inputs\n","        - list of tuples, each containing a PIL image and an integer label\n","        - number of tuples in the list == BATCH SIZE\n","\n","        Returns\n","        - batch of image tensors, batch of label tensors\n","        - size: (BATCH, CHANNEL, HEIGHT, WIDTH), (BATCH)\n","        \"\"\"\n","        batch_x, batch_y = [], []\n","\n","        ##### YOUR CODE #####\n","\n","        #####################\n","\n","        return batch_x, batch_y\n","    "]},{"cell_type":"code","execution_count":null,"id":"DpcJAAqWe-uM","metadata":{"id":"DpcJAAqWe-uM"},"outputs":[],"source":["\"\"\"\n","Plot some example images and class labels without applying data augmentation.\n","\n","Do NOT modify.\n","\"\"\"\n","\n","raw_test_dataset = CustomDataset(train=False, prob=0, data_dir=\"./CIFAR_10\")\n","raw_test_loader = DataLoader(dataset=raw_test_dataset, batch_size=16, shuffle=False, collate_fn=raw_test_dataset.collate_fn)\n","\n","plot_dataset(raw_test_loader)"]},{"cell_type":"code","execution_count":null,"id":"TCa4Ay4mWws9","metadata":{"id":"TCa4Ay4mWws9"},"outputs":[],"source":["\"\"\"\n","Same examples after applying data augmentation with 50% probability.\n","If your transform (c) and collate_fn (d) methods have been implemented well, some of the results should look different from the ones above.\n","\n","Do NOT modify.\n","\"\"\"\n","\n","train_dataset = CustomDataset(train=True, prob=0.5)\n","test_dataset = CustomDataset(train=False, prob=0.5)\n","\n","BATCH_SIZE = 64\n","\n","train_loader = DataLoader(dataset=train_dataset, \n","                          batch_size=BATCH_SIZE, \n","                          shuffle=True, \n","                          collate_fn=train_dataset.collate_fn)\n","test_loader = DataLoader(dataset=test_dataset, \n","                         batch_size=BATCH_SIZE, \n","                         shuffle=False, \n","                         collate_fn=test_dataset.collate_fn)\n","\n","plot_dataset(test_loader)"]},{"cell_type":"markdown","id":"2eeea83e","metadata":{"id":"2eeea83e"},"source":["# ConvNet Image Classification"]},{"cell_type":"code","execution_count":null,"id":"P0xUGjocL4OH","metadata":{"id":"P0xUGjocL4OH"},"outputs":[],"source":["### \n","# Question (e)\n","# Train your ConvNet to achieve test accuracy above 70%\n","# You can try or add other training options such as SGD or callbacks to schedule learning rates if you want.\n","###"]},{"cell_type":"code","execution_count":null,"id":"c4d35830","metadata":{"id":"c4d35830"},"outputs":[],"source":["class ConvNet(nn.Module):\n","    \"\"\"\n","    Builds a ConvNet model.\n","\n","    Question (e)\n","    - things that might be useful...\n","    - stack [Conv2D + Conv2D + MaxPool2D] at least three times, \n","    - follwed by at least three Linear layers.\n","    - 3x3 filter is enough, but feel free to use larger filter size.\n","    - channels used: [10, 32, 64, 128, 256, 512, 1024]\n","    - you can choose smaller or larger channel size as well.\n","    - The model may include BatchNormalization, regularizers, and Dropout, but they are not necessary.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"\n","        Define the layers that you would like to use in your model.\n","        \"\"\"\n","        super(ConvNet, self).__init__()\n","\n","        ##### YOUR CODE #####\n","\n","        #####################\n","    \n","    def forward(self, x):\n","        \"\"\"\n","        Apply forward pass of the given batch of input images.\n","        Inputs\n","        - x: batch of input images.\n","        Returns\n","        - softmax probabilites of the input image for each class label\n","        \"\"\"\n","\n","        ##### YOUR CODE #####\n","\n","        #####################\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"VbHlUuyTdhK2","metadata":{"id":"VbHlUuyTdhK2"},"outputs":[],"source":["\"\"\"\n","Make sure your runtime type is GPU and you are using PyTorch version higher than 1.8!\n","\n","Do NOT modify.\n","\"\"\"\n","\n","DEVICE = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"Using PyTorch version: {}, Device: {}\".format(torch.__version__, DEVICE))"]},{"cell_type":"code","execution_count":null,"id":"HA_mGa0orKj5","metadata":{"id":"HA_mGa0orKj5"},"outputs":[],"source":["\"\"\"\n","Load your customized model \"ConvNet\" and its training settings.\n","You may choose the number of epochs that you would like to train.\n","You might want to use different optimizers or learning rates.\n","\"\"\"\n","\n","EPOCHS = 10\n","model = ConvNet().to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss()\n","\n","print(model)"]},{"cell_type":"code","execution_count":null,"id":"sjeFjn0qrPRK","metadata":{"id":"sjeFjn0qrPRK"},"outputs":[],"source":["\"\"\"\n","Train your model \"ConvNet\" with the augmented CIFAR-10 dataset.\n","Upon successful training, test accuracy of your model should be above 70%.\n","\n","Do NOT modify.\n","\"\"\"\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train(model, train_loader, optimizer)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    print(\"\\n[EPOCH: {}], \\tModel: ConvNet, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n","        epoch, test_loss, test_accuracy))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}