{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"nCYO6dmGgefe"},"source":["# Colab Setup"]},{"cell_type":"code","metadata":{"id":"er0RD438gRLm"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jfeql_8sgnKJ"},"source":["\"\"\"\n","Change directory to where this file is located\n","\"\"\"\n","#%cd 'COPY&PASTE FILE DIRECTORY HERE'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sPEoabX-hGCh"},"source":["# Import Modules"]},{"cell_type":"code","metadata":{"id":"OyammZP8hI7P"},"source":["import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist.data_utils import load_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLxTNOvI5NHD"},"source":["#Utils"]},{"cell_type":"code","metadata":{"id":"xuQB6W2U5ZE2"},"source":["def tanh(z):\n","    \"\"\"\n","    Implement the tanh activation function.\n","    The method takes the input z and returns the output of the function.\n","\n","    Question (a)\n","\n","    \"\"\"\n","    ##### YOUR CODE #####\n","    return None\n","    #####################\n","\n","\n","def softmax(X):\n","    \"\"\"\n","    Implement the softmax function.\n","    The method takes the input X and returns the output of the function.\n","\n","    Question (a)\n","\n","    \"\"\"\n","    ##### YOUR CODE #####\n","    return None\n","    #####################\n","\n","\n","def load_batch(X, Y, batch_size, shuffle=True):\n","    \"\"\"\n","    Generates batches with the remainder dropped.\n","\n","    Do NOT modify this function\n","    \"\"\"\n","    if shuffle:\n","        permutation = np.random.permutation(X.shape[0])\n","        X = X[permutation, :]\n","        Y = Y[permutation, :]\n","    num_steps = int(X.shape[0])//batch_size\n","    step = 0\n","    while step<num_steps:\n","        X_batch = X[batch_size*step:batch_size*(step+1)]\n","        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n","        step+=1\n","        yield X_batch, Y_batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tsU8v_6khR30"},"source":["#2-Layer Neural Network"]},{"cell_type":"code","metadata":{"id":"mA5udiGmhRb5"},"source":["class TwoLayerNN:\n","    \"\"\" a neural network with 2 layers \"\"\"\n","\n","    def __init__(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        Do NOT modify this function.\n","        \"\"\"\n","        self.input_dim = input_dim\n","        self.num_hiddens = num_hiddens\n","        self.num_classes = num_classes\n","        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n","\n","    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        initializes parameters with Xavier Initialization.\n","\n","        Question (b)\n","        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization \n","        \n","        Inputs\n","        - input_dim\n","        - num_hiddens\n","        - num_classes\n","        Returns\n","        - params: a dictionary with the initialized parameters.\n","        \"\"\"\n","        params = {}\n","        ##### YOUR CODE #####\n","        params[\"W1\"] = None\n","        params[\"b1\"] = None\n","        params[\"W2\"] = None\n","        params[\"b2\"] = None\n","        #####################\n","        return params\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Define and perform the feed forward step of a two-layer neural network.\n","        Specifically, the network structue is given by\n","\n","          y = softmax(tanh(X W1 + b1) W2 + b2)\n","\n","        where X is the input matrix of shape (N, D), y is the class distribution matrix\n","        of shape (N, C), N is the number of examples (either the entire dataset or\n","        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n","\n","        Question (c)\n","        - ff_dict will be used to run backpropagation in backward method.\n","\n","        Inputs\n","        - X: the input matrix of shape (N, D)\n","\n","        Returns\n","        - y: the output of the model\n","        - ff_dict: a dictionary with all the fully connected units and activations.\n","        \"\"\"\n","        ff_dict = {}\n","        ##### YOUR CODE #####\n","\n","        #####################\n","        return y, ff_dict\n","\n","    def backward(self, X, Y, ff_dict):\n","        \"\"\"\n","        Performs backpropagation over the two-layer neural network, and returns\n","        a dictionary of gradients of all model parameters.\n","\n","        Question (d)\n","\n","        Inputs:\n","         - X: the input matrix of shape (B, D), where B is the number of examples\n","              in a mini-batch, D is the feature dimensionality.\n","         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n","              where B is the number of examples in a mini-batch, C is the number\n","              of classes.\n","         - ff_dict: the dictionary containing all the fully connected units and\n","              activations.\n","\n","        Returns:\n","         - grads: a dictionary containing the gradients of corresponding weights and biases.\n","        \"\"\"\n","        grads = {}\n","        ##### YOUR CODE #####\n","        grads[\"dW1\"] = None\n","        grads[\"db1\"] = None\n","        grads[\"dW2\"] = None\n","        grads[\"db2\"] = None\n","        #####################\n","        return grads\n","\n","\n","    def compute_loss(self, Y, Y_hat):\n","        \"\"\"\n","        Computes cross entropy loss.\n","\n","        Do NOT modify this function.\n","\n","        Inputs\n","            Y:\n","            Y_hat:\n","        Returns\n","            loss:\n","        \"\"\"\n","        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n","        return loss\n","\n","    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n","        \"\"\"\n","        Runs mini-batch gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X\n","        - Y\n","        - X_val\n","        - Y_Val\n","        - lr\n","        - n_epochs\n","        - batch_size\n","        - log_interval\n","        \"\"\"\n","        for epoch in range(n_epochs):\n","            for X_batch, Y_batch in load_batch(X, Y, batch_size):\n","                self.train_step(X_batch, Y_batch, batch_size, lr)\n","            if epoch % log_interval==0:\n","                Y_hat, ff_dict = self.forward(X)\n","                train_loss = self.compute_loss(Y, Y_hat)\n","                train_acc = self.evaluate(Y, Y_hat)\n","                Y_hat, ff_dict = self.forward(X_val)\n","                valid_loss = self.compute_loss(Y_val, Y_hat)\n","                valid_acc = self.evaluate(Y_val, Y_hat)\n","                print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n","                      format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n","\n","    def train_step(self, X_batch, Y_batch, batch_size, lr):\n","        \"\"\"\n","        Updates the parameters using gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X_batch\n","        - Y_batch\n","        - batch_size\n","        - lr\n","        \"\"\"\n","        _, ff_dict = self.forward(X_batch)\n","        grads = self.backward(X_batch, Y_batch, ff_dict)\n","        self.params[\"W1\"] -= lr * grads[\"dW1\"]/batch_size\n","        self.params[\"b1\"] -= lr * grads[\"db1\"]/batch_size\n","        self.params[\"W2\"] -= lr * grads[\"dW2\"]/batch_size\n","        self.params[\"b2\"] -= lr * grads[\"db2\"]/batch_size\n","\n","    def evaluate(self, Y, Y_hat):\n","        \"\"\"\n","        Computes classification accuracy.\n","        \n","        Do NOT modify this function\n","\n","        Inputs\n","        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n","             where C is the number of classes.\n","        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n","             where C is the number of classes.\n","\n","        Returns\n","            accuracy: the classification accuracy in float\n","        \"\"\"        \n","        classes_pred = np.argmax(Y_hat, axis=1)\n","        classes_gt = np.argmax(Y, axis=1)\n","        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n","        return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XXM2lWhtDYC6"},"source":["#Load MNIST"]},{"cell_type":"code","metadata":{"id":"48ooR6YIxYhC"},"source":["X_train, Y_train, X_test, Y_test = load_data()\n","\n","idxs = np.arange(len(X_train))\n","np.random.shuffle(idxs)\n","split_idx = int(np.ceil(len(idxs)*0.8))\n","X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n","X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n","print()\n","print('Set validation data aside')\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', Y_train.shape)\n","print('Validation data shape: ', X_valid.shape)\n","print('Validation labels shape: ', Y_valid.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tzw-D4Zr5xoi"},"source":["#Training & Evaluation"]},{"cell_type":"code","metadata":{"id":"IlnC_rerHPaN"},"source":["### \n","# Question (e)\n","# Tune the hyperparameters with validation data, \n","# and print the results by running the lines below.\n","###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTCqVT4S0Tm5"},"source":["# model instantiation\n","model = TwoLayerNN(input_dim=784, num_hiddens=64, num_classes=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6cWb6xg0NxOs"},"source":["# train the model\n","lr, n_epochs, batch_size = 2.0, 20, 256\n","model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpPsAlXU0T_Z"},"source":["# evalute the model on test data\n","Y_hat, _ = model.forward(X_test)\n","test_loss = model.compute_loss(Y_test, Y_hat)\n","test_acc = model.evaluate(Y_test, Y_hat)\n","print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5hh1PZpk_g0I"},"source":["# Extra Credit (Optional)"]},{"cell_type":"code","metadata":{"id":"0R0n6y9_AgXc"},"source":["def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n","    \"\"\"\n","    initializes parameters with He Initialization.\n","\n","    Question (f)\n","    - refer to https://paperswithcode.com/method/he-initialization for He initialization \n","    \n","    Inputs\n","    - input_dim\n","    - num_hiddens\n","    - num_classes\n","    Returns\n","    - params: a dictionary with the initialized parameters.\n","    \"\"\"\n","\n","    params = {}\n","    ##### YOUR CODE #####\n","\n","    #####################\n","    return params\n","\n","def forward_relu(self, X):\n","    \"\"\"\n","    Defines and performs the feed forward step of a two-layer neural network.\n","    Specifically, the network structue is given by\n","\n","        y = softmax(relu(X W1 + b1) W2 + b2)\n","\n","    where X is the input matrix of shape (N, D), y is the class distribution matrix\n","    of shape (N, C), N is the number of examples (either the entire dataset or\n","    a mini-batch), D is the feature dimensionality, and C is the number of classes.\n","\n","    Question (f)\n","\n","    Inputs\n","        X: the input matrix of shape (N, D)\n","\n","    Returns\n","        y: the output of the model\n","        ff_dict: a dictionary containing all the fully connected units and activations.\n","    \"\"\"\n","\n","    ff_dict = {}        \n","    ##### YOUR CODE #####\n","\n","    #####################\n","    return y, ff_dict\n","\n","def backward_relu(self, X, Y, ff_dict):\n","    \"\"\"\n","    Performs backpropagation over the two-layer neural network, and returns\n","    a dictionary of gradients of all model parameters.\n","\n","    Question (f)\n","\n","    Inputs:\n","        - X: the input matrix of shape (B, D), where B is the number of examples\n","            in a mini-batch, D is the feature dimensionality.\n","        - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n","            where B is the number of examples in a mini-batch, C is the number\n","            of classes.\n","        - ff_dict: the dictionary containing all the fully connected units and\n","            activations.\n","\n","    Returns:\n","        - grads: a dictionary containing the gradients of corresponding weights\n","            and biases.\n","    \"\"\"\n","\n","    grads = {}\n","    ##### YOUR CODE #####\n","\n","    #####################\n","    return grads\n","\n","TwoLayerNNRelu = copy.copy(TwoLayerNN)\n","TwoLayerNNRelu.initialize_parameters = initialize_parameters\n","TwoLayerNNRelu.feed_forward = forward_relu\n","TwoLayerNNRelu.back_propagate = backward_relu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qY3T98XvEIP"},"source":["### \n","# Question (f)\n","# Tune the hyperparameters with validation data,\n","# and print the results by running the lines below.\n","###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-jJRXqsBxzh"},"source":["# model instantiation\n","model_relu = TwoLayerNNRelu(input_dim=784, num_hiddens=64, num_classes=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EC8f80a0w53m"},"source":["# train the model\n","lr, n_epochs, batch_size = 1.5, 20, 256\n","history = model_relu.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4i__6TfpCqOc"},"source":["Y_hat, _ = model_relu.forward(X_test)\n","test_loss = model_relu.compute_loss(Y_test, Y_hat)\n","test_acc = model_relu.evaluate(Y_test, Y_hat)\n","print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"],"execution_count":null,"outputs":[]}]}